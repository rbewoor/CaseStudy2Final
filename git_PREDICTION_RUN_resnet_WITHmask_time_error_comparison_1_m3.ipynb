{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.6.9"
    },
    "colab": {
      "name": "PREDICTION_RUN_resnet_WITHmask_time_error_comparison-1_m3.ipynb",
      "provenance": [],
      "collapsed_sections": [
        "Xv5DkxADciIb",
        "aHkW0KDeciIo",
        "smqKCkuKciIu",
        "K3bXB9TkciIy",
        "sK9R1tcwciI5",
        "pgbVYdESciI_"
      ]
    },
    "accelerator": "GPU"
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "1_WIzOceciHq",
        "colab_type": "text"
      },
      "source": [
        "# PREDICTION comparison using the saved models:\n",
        "\n",
        "## resnet , WITH mask\n",
        "\n",
        "## running only 3rd model = 'model_kagg-rohit-center-resnet-1_ep4-loss-focal-mask.pth'\n",
        "\n",
        "### 1) Time\n",
        "\n",
        "### 2) MSE for the outputs"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "_cell_guid": "b1076dfc-b9ad-4769-8c92-a6c4dae69d19",
        "_kg_hide-input": true,
        "_kg_hide-output": true,
        "_uuid": "8f2839f25d086af736a60e9eeb907d3b93b6e0e5",
        "nbpresent": {
          "id": "6a63605e-0508-479a-94d0-edc7195a51a2"
        },
        "id": "FQVYgkY4ciHr",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "import numpy as np\n",
        "import pandas as pd\n",
        "import cv2\n",
        "from tqdm import tqdm_notebook as tqdm\n",
        "import matplotlib.pyplot as plt\n",
        "import seaborn as sns\n",
        "from functools import reduce\n",
        "import os\n",
        "from sklearn.model_selection import train_test_split\n",
        "from scipy.optimize import minimize\n",
        "from tqdm.auto import tqdm as tq\n",
        "import torch\n",
        "import torch.nn as nn\n",
        "import torch.nn.functional as F\n",
        "import torch.optim as optim\n",
        "from torch.optim import lr_scheduler\n",
        "from torch.utils.data import Dataset, DataLoader\n",
        "from torchvision import models\n",
        "from torchvision import transforms, utils\n",
        "\n",
        "import albumentations as alb"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "z7HY1MrAciHv",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 34
        },
        "outputId": "79a7227e-6637-4d11-877b-d5cc5025f1a4"
      },
      "source": [
        "# Gets the GPU if there is one, otherwise the cpu\n",
        "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
        "#device = torch.device(\"cpu\")\n",
        "print(device)\n"
      ],
      "execution_count": 2,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "cuda\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "wHA3dEA2ciH0",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "USEMASK = True\n",
        "\n",
        "KAGGLE = False\n",
        "COLAB = True\n",
        "\n",
        "# SPECIFY THE MODEL PATH BELOW BEFORE RUNNING"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "s0LdJ2mpciH3",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "## KAGGLE and COLAB flags should not be True at same time - fail if this is case\n",
        "assert not (KAGGLE and COLAB), \"both KAGGLE and COLAB runs cannot be true\""
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "KYrreWUpciH7",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 326
        },
        "outputId": "0c7176e1-dc6b-4cdf-b89b-fbc9b7737f5a"
      },
      "source": [
        "if KAGGLE and not COLAB:          #  kaggle run\n",
        "    HOMEDIR = r'../input/pku-autonomous-driving/'\n",
        "    OUTDIR = r'./'\n",
        "    model_path_dir = None\n",
        "    \n",
        "elif COLAB and not KAGGLE:         # google colab\n",
        "    from google.colab import drive\n",
        "    drive.mount('/content/drive')\n",
        "    #\n",
        "    HOMEDIR = r'/content/drive/My Drive/baidu/pku-autonomous-driving/'\n",
        "    OUTDIR = r'./content/drive/My Drive/baidu/pku-autonomous-driving/outputROHIT/'\n",
        "    model_path_dir = r'/content/drive/My Drive/baidu/Models/Cent-Resnet18_WITHmask/'\n",
        "\n",
        "elif not KAGGLE and not COLAB:\n",
        "    HOMEDIR = r\"/media/rohit/DATA/EverythingD/01SRH-BDBA Acads/CaseStudy2/Kaggle-PekingAutonomousDriving/pku-autonomous-driving/\"\n",
        "    OUTDIR = r\"/media/rohit/DATA/EverythingD/01SRH-BDBA Acads/CaseStudy2/Kaggle-PekingAutonomousDriving/pku-autonomous-driving/output/\"\n",
        "    model_path_dir = r'/home/rohit/SRH/CaseStudy2/Models/Cent-Resnet18_WITHmask/'\n",
        "\n",
        "#\n",
        "if COLAB: print(f\"COLAB\")\n",
        "if KAGGLE: print(f\"KAGGLE\")\n",
        "if not (KAGGLE and COLAB): print(f\"laptop, not CLOUD\\n\")\n",
        "else: print(f\"on CLOUD\\n\")\n",
        "\n",
        "print(f\"HOMEDIR =\\n{HOMEDIR}\\n\\nOUTDIR=\\n{OUTDIR}\\n\\nmodel_path_dir =\\n{model_path_dir}\\n\")"
      ],
      "execution_count": 5,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Go to this URL in a browser: https://accounts.google.com/o/oauth2/auth?client_id=947318989803-6bn6qk8qdgf4n4g3pfee6491hc0brc4i.apps.googleusercontent.com&redirect_uri=urn%3aietf%3awg%3aoauth%3a2.0%3aoob&response_type=code&scope=email%20https%3a%2f%2fwww.googleapis.com%2fauth%2fdocs.test%20https%3a%2f%2fwww.googleapis.com%2fauth%2fdrive%20https%3a%2f%2fwww.googleapis.com%2fauth%2fdrive.photos.readonly%20https%3a%2f%2fwww.googleapis.com%2fauth%2fpeopleapi.readonly\n",
            "\n",
            "Enter your authorization code:\n",
            "··········\n",
            "Mounted at /content/drive\n",
            "COLAB\n",
            "laptop, not CLOUD\n",
            "\n",
            "HOMEDIR =\n",
            "/content/drive/My Drive/baidu/pku-autonomous-driving/\n",
            "\n",
            "OUTDIR=\n",
            "./content/drive/My Drive/baidu/pku-autonomous-driving/outputROHIT/\n",
            "\n",
            "model_path_dir =\n",
            "/content/drive/My Drive/baidu/Models/Cent-Resnet18_WITHmask/\n",
            "\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "QsSNnXewciH_",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "assert model_path_dir is not None, \"model path directory is not specified\""
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "nbpresent": {
          "id": "0024d699-064c-4fb3-a3c2-04bb8c3c65ea"
        },
        "id": "FjR_65PTciID",
        "colab_type": "text"
      },
      "source": [
        "# Basic info loading\n",
        "\n",
        "## data, camera matrix specification\n",
        "\n",
        "\n",
        "## train.csv   has   ImageId, PredictionString\n",
        "\n",
        "## Prediction String    model# yaw pitch roll x y z  and the same for multiple cars"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "_cell_guid": "79c7e3d0-c299-4dcb-8224-4455121ee9b0",
        "_kg_hide-input": true,
        "_uuid": "d629ff2d2480ee46fbb7e2d37f6b5fab8052498a",
        "nbpresent": {
          "id": "63ca552d-09e4-4632-bda9-20e51805d3ee"
        },
        "id": "0FNP9TwKciIE",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "dfTrain = pd.read_csv(HOMEDIR + 'train.csv')\n",
        "dfTest = pd.read_csv(HOMEDIR + 'sample_submission.csv')\n",
        "\n",
        "# From camera.zip\n",
        "camera_matrix = np.array([[2304.5479, 0,  1686.2379],\n",
        "                          [0, 2305.8757, 1354.9849],\n",
        "                          [0, 0, 1]], dtype=np.float32)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "oOL7tfgociII",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 204
        },
        "outputId": "dd870aa5-36e5-4562-f088-289378238f19"
      },
      "source": [
        "dfTrain.head()"
      ],
      "execution_count": 8,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/html": [
              "<div>\n",
              "<style scoped>\n",
              "    .dataframe tbody tr th:only-of-type {\n",
              "        vertical-align: middle;\n",
              "    }\n",
              "\n",
              "    .dataframe tbody tr th {\n",
              "        vertical-align: top;\n",
              "    }\n",
              "\n",
              "    .dataframe thead th {\n",
              "        text-align: right;\n",
              "    }\n",
              "</style>\n",
              "<table border=\"1\" class=\"dataframe\">\n",
              "  <thead>\n",
              "    <tr style=\"text-align: right;\">\n",
              "      <th></th>\n",
              "      <th>ImageId</th>\n",
              "      <th>PredictionString</th>\n",
              "    </tr>\n",
              "  </thead>\n",
              "  <tbody>\n",
              "    <tr>\n",
              "      <th>0</th>\n",
              "      <td>ID_8a6e65317</td>\n",
              "      <td>16 0.254839 -2.57534 -3.10256 7.96539 3.20066 ...</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>1</th>\n",
              "      <td>ID_337ddc495</td>\n",
              "      <td>66 0.163988 0.192169 -3.12112 -3.17424 6.55331...</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>2</th>\n",
              "      <td>ID_a381bf4d0</td>\n",
              "      <td>43 0.162877 0.00519276 -3.02676 2.1876 3.53427...</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>3</th>\n",
              "      <td>ID_7c4a3e0aa</td>\n",
              "      <td>43 0.126957 -3.04442 -3.10883 -14.738 24.6389 ...</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>4</th>\n",
              "      <td>ID_8b510fad6</td>\n",
              "      <td>37 0.16017 0.00862796 -3.0887 -3.04548 3.4977 ...</td>\n",
              "    </tr>\n",
              "  </tbody>\n",
              "</table>\n",
              "</div>"
            ],
            "text/plain": [
              "        ImageId                                   PredictionString\n",
              "0  ID_8a6e65317  16 0.254839 -2.57534 -3.10256 7.96539 3.20066 ...\n",
              "1  ID_337ddc495  66 0.163988 0.192169 -3.12112 -3.17424 6.55331...\n",
              "2  ID_a381bf4d0  43 0.162877 0.00519276 -3.02676 2.1876 3.53427...\n",
              "3  ID_7c4a3e0aa  43 0.126957 -3.04442 -3.10883 -14.738 24.6389 ...\n",
              "4  ID_8b510fad6  37 0.16017 0.00862796 -3.0887 -3.04548 3.4977 ..."
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 8
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "nbpresent": {
          "id": "10342326-4cfb-48ed-9384-ad869c9cecba"
        },
        "id": "L7NRZAr6ciIL",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 51
        },
        "outputId": "be32880f-93b5-4842-8cc3-2e29389bba0b"
      },
      "source": [
        "bad_images_list = [\"ID_1a5a10365\",\"ID_4d238ae90\",\"ID_408f58e9f\",\"ID_bb1d991f6\",\"ID_c44983aeb\"]\n",
        "for bad_id in bad_images_list:\n",
        "    #plt.imshow( cv2.imread(HOMEDIR + 'train_images/' + bad_id + '.jpg')[:,:,::-1] )\n",
        "    #plt.show()\n",
        "    pass\n",
        "print(f\"full df len = {len(dfTrain)}\")\n",
        "drop_dfTrain = dfTrain.set_index(\"ImageId\").drop( index=bad_images_list )\n",
        "dfTrain = drop_dfTrain.reset_index()\n",
        "print(f\"post removal of bad entries len = {len(dfTrain)}\")"
      ],
      "execution_count": 9,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "full df len = 4262\n",
            "post removal of bad entries len = 4257\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "qMSSHzWCciIP",
        "colab_type": "text"
      },
      "source": [
        "# Set swtich for very small data run"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "feM5-omnciIR",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "TEST_SWITCH_ON = False"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "rU8jNiSRciIU",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "if TEST_SWITCH_ON:\n",
        "    dfTrain = dfTrain[:20]\n",
        "    dfTest  = dfTest[:6]"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "nbpresent": {
          "id": "899ba2fa-a9a6-4b4a-b3d8-8a1fb11d037b"
        },
        "id": "pRZTKxqJciIY",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "def str2coords(ps, names=['id', 'yaw', 'pitch', 'roll', 'x', 'y', 'z']):\n",
        "# from the prediction string entries, create a dict of each car data\n",
        "    coords = []\n",
        "    for ps_entry in np.array(ps.split()).reshape([-1, 7]):\n",
        "        dictval = dict(zip(names, ps_entry.astype('float')))\n",
        "        coords.append(dictval)\n",
        "        if 'id' in coords[-1]:\n",
        "            coords[-1]['id'] = int(coords[-1]['id'])\n",
        "    return coords"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "nbpresent": {
          "id": "63a68ff2-3fd7-4c58-aa77-c97b5a309926"
        },
        "id": "Xv5DkxADciIb",
        "colab_type": "text"
      },
      "source": [
        "# 2D Visualization"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "fYodiEugciIc",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "def rotate(x, angle):\n",
        "    x = x + angle\n",
        "    x = x - (x + np.pi) // (2 * np.pi) * 2 * np.pi\n",
        "    return x"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "g_akax29ciIf",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "def read_image(path):\n",
        "    img = cv2.imread(path)\n",
        "    #img = np.array(img[:, :, ::-1]) # alternative way to convert BGR to RGB\n",
        "    img = cv2.cvtColor(img, cv2.COLOR_BGR2RGB) \n",
        "    return img"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "nbpresent": {
          "id": "64874f90-1066-4d31-91a8-76d74e9024ac"
        },
        "id": "sJ7GXlbxciIj",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "def convert_to_img_coords(ps):\n",
        "# convert the camera coords x,y,z to the image coords\n",
        "    coords = str2coords(ps)\n",
        "    x_list = [c['x'] for c in coords]\n",
        "    y_list = [c['y'] for c in coords]\n",
        "    z_list = [c['z'] for c in coords]\n",
        "    P = np.array(list(zip(x_list, y_list, z_list))).T\n",
        "    img_p = np.dot(camera_matrix, P).T\n",
        "    img_p[:, 0] /= img_p[:, 2]\n",
        "    img_p[:, 1] /= img_p[:, 2]\n",
        "    img_x_list = img_p[:, 0]\n",
        "    img_y_list = img_p[:, 1]\n",
        "    img_z_list = img_p[:, 2]\n",
        "    return img_x_list, img_y_list"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "nbpresent": {
          "id": "c3ed48e9-2bea-479e-8e38-f8743b00c6ec"
        },
        "id": "aHkW0KDeciIo",
        "colab_type": "text"
      },
      "source": [
        "# 3D Visualization"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "_kg_hide-input": true,
        "nbpresent": {
          "id": "0620bdfe-a8cc-4f28-a488-faf81a748072"
        },
        "id": "8iXVVrCmciIp",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "from math import sin, cos\n",
        "\n",
        "# convert euler angle to rotation matrix\n",
        "def euler_to_Rot(yaw, pitch, roll):\n",
        "    Y = np.array([[cos(yaw), 0, sin(yaw)],\n",
        "                  [0, 1, 0],\n",
        "                  [-sin(yaw), 0, cos(yaw)]])\n",
        "    P = np.array([[1, 0, 0],\n",
        "                  [0, cos(pitch), -sin(pitch)],\n",
        "                  [0, sin(pitch), cos(pitch)]])\n",
        "    R = np.array([[cos(roll), -sin(roll), 0],\n",
        "                  [sin(roll), cos(roll), 0],\n",
        "                  [0, 0, 1]])\n",
        "    return np.dot(Y, np.dot(P, R))"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "_kg_hide-input": true,
        "nbpresent": {
          "id": "4b22780a-18c3-4d1e-92e2-57f0f41c385e"
        },
        "id": "G826hydmciIs",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "def draw_line(image, points):\n",
        "    color = (255, 0, 0)\n",
        "    cv2.line(image, tuple(points[0][:2]), tuple(points[3][:2]), color, 16)\n",
        "    cv2.line(image, tuple(points[0][:2]), tuple(points[1][:2]), color, 16)\n",
        "    cv2.line(image, tuple(points[1][:2]), tuple(points[2][:2]), color, 16)\n",
        "    cv2.line(image, tuple(points[2][:2]), tuple(points[3][:2]), color, 16)\n",
        "    return image\n",
        "\n",
        "\n",
        "def draw_points(image, points):\n",
        "    for (p_x, p_y, p_z) in points:\n",
        "        cv2.circle(image, (p_x, p_y), int(1000 / p_z), (0, 255, 0), -1)\n",
        "#         if p_x > image.shape[1] or p_y > image.shape[0]:\n",
        "#             print('Point', p_x, p_y, 'is out of image with shape', image.shape)\n",
        "    return image"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "smqKCkuKciIu",
        "colab_type": "text"
      },
      "source": [
        "# Average ratios of the spanX, spanY and spanZ  by groups decided by me\n",
        "\n",
        "GroupNo based on Z / X value bin.\n",
        "GroupNo     Xspan       Yspan       Zspan       Yspan/Xspan     Zspan/Xspan\n",
        "1           1.96        1.58        3.78\t\t0.80            1.92\n",
        "2           2.08        1.65        4.49        0.80            2.15\n",
        "3           2.03        1.57        4.86        0.78            2.38\n",
        "\n",
        "average of all these\n",
        "GroupNo     Xspan       Yspan       Zspan       Yspan/Xspan     Zspan/Xspan\n",
        "            1.00        0.79        2.15        0.79            2.15"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "nbpresent": {
          "id": "0bf0b717-12ce-42fa-855b-eb00090d9571"
        },
        "id": "Z3JOVZLDciIv",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "def visualize(img, coords):\n",
        "    ## want to visualize \n",
        "    x_l = 1.00\n",
        "    y_l = 0.79\n",
        "    z_l = 2.15\n",
        "    \n",
        "    img = img.copy()\n",
        "    for point in coords:\n",
        "        # Get values\n",
        "        x, y, z = point['x'], point['y'], point['z']\n",
        "        ## the yaw and pitch is interchanged in the data provided\n",
        "        yaw, pitch, roll = -point['pitch'], -point['yaw'], -point['roll']\n",
        "        # Math\n",
        "        Rt = np.eye(4)\n",
        "        t = np.array([x, y, z])\n",
        "        Rt[:3, 3] = t\n",
        "        Rt[:3, :3] = euler_to_Rot(yaw, pitch, roll).T\n",
        "        Rt = Rt[:3, :]\n",
        "        P = np.array([[x_l, -y_l, -z_l, 1],\n",
        "                      [x_l, -y_l, z_l, 1],\n",
        "                      [-x_l, -y_l, z_l, 1],\n",
        "                      [-x_l, -y_l, -z_l, 1],\n",
        "                      [0, 0, 0, 1]]).T\n",
        "        img_cor_points = np.dot(camera_matrix, np.dot(Rt, P))\n",
        "        img_cor_points = img_cor_points.T\n",
        "        img_cor_points[:, 0] /= img_cor_points[:, 2]\n",
        "        img_cor_points[:, 1] /= img_cor_points[:, 2]\n",
        "        img_cor_points = img_cor_points.astype(int)\n",
        "        # Drawing\n",
        "        img = draw_line(img, img_cor_points)\n",
        "        img = draw_points(img, img_cor_points[-1:])\n",
        "    \n",
        "    return img"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "nbpresent": {
          "id": "6f1121d8-0353-453f-8a99-b1230811b173"
        },
        "id": "K3bXB9TkciIy",
        "colab_type": "text"
      },
      "source": [
        "# Steps to preprocess input image"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "nbpresent": {
          "id": "04405c31-3249-40d8-963c-0a1006947fbb"
        },
        "id": "sw3sBAueciIz",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "# Original dimensions of the Train and Test images\n",
        "ORIG_W = 3384\n",
        "ORIG_H = 2710\n",
        "\n",
        "# The dimensions we want to use for processing: keeping ratio of width:height = 1:4\n",
        "IMG_WIDTH = 2048\n",
        "IMG_HEIGHT = 512\n",
        "MARGIN_W = ORIG_W // 4  # 846\n",
        "\n",
        "MODEL_SCALE = 8  # mask shrink rate\n",
        "\n",
        "FX, FY = 2304.5479,  2305.8757\n",
        "CX, CY = 1686.2379, 1354.9849\n",
        "def XYZ2UV(x,y,z):\n",
        "    u = FX * x / z + CX\n",
        "    v = FY * y / z + CY\n",
        "    return u,v\n",
        "def UVZ2XY(u,v,z):\n",
        "    x = z * (u - CX) / FX\n",
        "    y = z * (v - CY) / FY\n",
        "    return x,y\n",
        "\n",
        "#\n",
        "# u is horizontal dimension and v is vertical dimension\n",
        "#\n",
        "def VU2maskVU(v,u):  \n",
        "    mask_V = (v - ORIG_H // 2) * IMG_HEIGHT / (ORIG_H // 2) / MODEL_SCALE\n",
        "    mask_U = (u + MARGIN_W) * IMG_WIDTH  / (ORIG_W + 2*MARGIN_W) / MODEL_SCALE\n",
        "    return mask_V, mask_U\n",
        "def maskVU2VU(mask_v_float, mask_u_float):\n",
        "    v = ORIG_H // 2 + mask_v_float * MODEL_SCALE / IMG_HEIGHT * (ORIG_H // 2)\n",
        "    u = mask_u_float * MODEL_SCALE * (ORIG_W + 2*MARGIN_W) / IMG_WIDTH - MARGIN_W\n",
        "    return v, u\n",
        "\n",
        "## assertion usage\n",
        "REGR_TARGETS = sorted( [\"yaw\",\"pitch_sin\", \"pitch_cos\", \"roll\", \"udiff\", \"vdiff\", \"z\"] )\n",
        "def _regr_preprocess(regr_dict, vdiff, udiff):\n",
        "    \"\"\" vdiff(h orientation), udiff is regression target \"\"\"\n",
        "    regr_dict[\"vdiff\"] = vdiff\n",
        "    regr_dict[\"udiff\"] = udiff\n",
        "\n",
        "    # Roll\n",
        "    regr_dict['roll'] = rotate(regr_dict['roll'], np.pi)\n",
        "    \n",
        "    # Pitch\n",
        "    regr_dict['pitch_sin'] = sin(regr_dict['pitch'])\n",
        "    regr_dict['pitch_cos'] = cos(regr_dict['pitch'])\n",
        "\n",
        "    # Regress log(Z)\n",
        "    regr_dict[\"z\"] = np.log(regr_dict[\"z\"])\n",
        "    \n",
        "    regr_dict.pop('x')\n",
        "    regr_dict.pop('y')\n",
        "    regr_dict.pop('pitch')\n",
        "    regr_dict.pop('id')\n",
        "    return regr_dict\n",
        "\n",
        "def _regr_back(regr_dict, mask_V_pos, mask_U_pos):\n",
        "    # convert log(z) back to z\n",
        "    regr_dict[\"z\"] = np.exp(regr_dict[\"z\"])\n",
        "\n",
        "    _v, _u = maskVU2VU( mask_V_pos + regr_dict[\"vdiff\"], mask_U_pos + regr_dict[\"udiff\"] )\n",
        "    regr_dict[\"x\"], regr_dict[\"y\"] = UVZ2XY(_u, _v, regr_dict[\"z\"])\n",
        "\n",
        "    regr_dict['roll'] = rotate(regr_dict['roll'], -np.pi)\n",
        "\n",
        "    ## Pitch\n",
        "    pitch_sin = regr_dict['pitch_sin'] / np.sqrt(regr_dict['pitch_sin']**2 + regr_dict['pitch_cos']**2)\n",
        "    pitch_cos = regr_dict['pitch_cos'] / np.sqrt(regr_dict['pitch_sin']**2 + regr_dict['pitch_cos']**2)\n",
        "    regr_dict['pitch'] = np.arccos(pitch_cos) * np.sign(pitch_sin)\n",
        "    \n",
        "    return regr_dict\n",
        "\n",
        "def preprocess_image(img):\n",
        "    img = img[img.shape[0] // 2:]\n",
        "    bg = np.ones_like(img) * img.mean(1, keepdims=True).astype(img.dtype)\n",
        "    bg = bg[:, :MARGIN_W]\n",
        "    img = np.concatenate([bg, img, bg], 1)\n",
        "    img = cv2.resize(img, (IMG_WIDTH, IMG_HEIGHT))\n",
        "    return (img / 255).astype('float32')\n",
        "def preprocess_mask_image(img):  # 上関数とといっしょに編集するように注意\n",
        "    img = img[img.shape[0] // 2:]\n",
        "    bg = np.zeros_like(img).astype(img.dtype)\n",
        "    bg = bg[:, :img.shape[1] // 4]\n",
        "    img = np.concatenate([bg, img, bg], 1)\n",
        "    img = cv2.resize(img, (IMG_WIDTH, IMG_HEIGHT))  # linear interpolate\n",
        "    return (img / 255).astype('float32')\n",
        "\n",
        "# https://github.com/xingyizhou/CenterNet/blob/819e0d0dde02f7b8cb0644987a8d3a370aa8206a/src/lib/utils/image.py\n",
        "# heatmap: H, W\n",
        "# center : X(w direction), Y(H direction)\n",
        "##################### mu_x = int(center[0] + 0.5) CAUSES BUG ##################################\n",
        "\n",
        "def draw_msra_gaussian(heatmap, center, sigma):\n",
        "    # tmp_size = sigma * 3\n",
        "    tmp_size = np.ceil(sigma * 3).astype(int)  # tmp_size should be int for readability ( and to remove bug ? )\n",
        "    mu_x = int(center[0])\n",
        "    mu_y = int(center[1])\n",
        "    w, h = heatmap.shape[0], heatmap.shape[1]\n",
        "    ul = [int(mu_x - tmp_size), int(mu_y - tmp_size)]\n",
        "    br = [int(mu_x + tmp_size + 1), int(mu_y + tmp_size + 1)]\n",
        "    if ul[0] >= h or ul[1] >= w or br[0] < 0 or br[1] < 0:\n",
        "        return heatmap\n",
        "    size = 2 * tmp_size + 1\n",
        "    x = np.arange(0, size, 1, np.float32)\n",
        "    y = x[:, np.newaxis]\n",
        "    x0 = y0 = size // 2\n",
        "    g = np.exp(- ((x - x0) ** 2 + (y - y0) ** 2) / (2 * sigma ** 2))\n",
        "    g_x = max(0, -ul[0]), min(br[0], h) - ul[0]\n",
        "    g_y = max(0, -ul[1]), min(br[1], w) - ul[1]\n",
        "    img_x = max(0, ul[0]), min(br[0], h)\n",
        "    img_y = max(0, ul[1]), min(br[1], w)\n",
        "    heatmap[img_y[0]:img_y[1], img_x[0]:img_x[1]] = np.maximum(\n",
        "      heatmap[img_y[0]:img_y[1], img_x[0]:img_x[1]],\n",
        "      g[g_y[0]:g_y[1], g_x[0]:g_x[1]])\n",
        "    return heatmap\n",
        "\n",
        "def make_heatmap(m, v_arr, u_arr, z_arr):\n",
        "    for v,u,z in zip(v_arr, u_arr, z_arr):\n",
        "        # sigma = 1000 / 3.  / z / MODEL_SCALE\n",
        "        sigma = 800 / 3.  / z / MODEL_SCALE\n",
        "        m = draw_msra_gaussian(m, (u,v), sigma)\n",
        "    return m\n",
        "        \n",
        "\n",
        "def get_mask_and_regr(img, labels):\n",
        "    mask = np.zeros([IMG_HEIGHT // MODEL_SCALE, IMG_WIDTH // MODEL_SCALE], dtype='float32')\n",
        "    regr = np.zeros([IMG_HEIGHT // MODEL_SCALE, IMG_WIDTH // MODEL_SCALE, 7], dtype='float32')\n",
        "    coords = str2coords(labels)\n",
        "    xs, ys = convert_to_img_coords(labels)\n",
        "    z_arr = [e[\"z\"] for e in coords]\n",
        "    \n",
        "    mask_V_arr_float, mask_U_arr_float = VU2maskVU( ys, xs )\n",
        "\n",
        "    # use floor floowing paper\n",
        "    mask_V_arr = np.floor( mask_V_arr_float ).astype('int')\n",
        "    mask_U_arr = np.floor( mask_U_arr_float ).astype('int')\n",
        "    mask_V_diff = mask_V_arr_float - mask_V_arr\n",
        "    mask_U_diff = mask_U_arr_float - mask_U_arr\n",
        "\n",
        "    # make heatmap\n",
        "    mask = make_heatmap(mask, mask_V_arr, mask_U_arr, z_arr)\n",
        "    \n",
        "    for mask_V,mask_U, vdiff,udiff, regr_dict in zip(mask_V_arr,mask_U_arr,mask_V_diff,mask_U_diff, coords):\n",
        "        if mask_V >= 0 and mask_V < IMG_HEIGHT // MODEL_SCALE and mask_U >= 0 and mask_U < IMG_WIDTH // MODEL_SCALE:\n",
        "            regr_dict = _regr_preprocess(regr_dict, vdiff, udiff)\n",
        "            regr[mask_V, mask_U] = [regr_dict[n] for n in sorted(regr_dict)]\n",
        "    return mask, regr"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "wgiEsvu2ciI2",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "def vis_mask(img, mask):\n",
        "    _mm = np.repeat( np.repeat(mask, 8, axis=0), 8, axis=1 )[:,:, None]\n",
        "    _mm = np.repeat(_mm, 3, axis=-1)\n",
        "    _mm [:,:,1] = 0 ; _mm[:,:,2] = 0\n",
        "    \n",
        "    tmp =  np.clip( 0.8 * img + 0.4 * _mm, 0,1)\n",
        "    tmp[ _mm[:,:,0]==1 ] = [0,1,1]\n",
        "    plt.figure(figsize=(16,16))\n",
        "    plt.imshow( tmp , alpha=0.3)\n",
        "    plt.grid()\n",
        "    plt.show()\n"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "nbpresent": {
          "id": "79072029-2fef-4ae9-a071-ab86ae083573"
        },
        "id": "sK9R1tcwciI5",
        "colab_type": "text"
      },
      "source": [
        "# PyTorch Dataset"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "_kg_hide-input": true,
        "_kg_hide-output": true,
        "nbpresent": {
          "id": "89e7fbe4-09dc-46c2-9498-582a0b959727"
        },
        "id": "JA6DR8mXciI6",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "class CarDataset(Dataset):\n",
        "    \"\"\"Car dataset.\"\"\"\n",
        "\n",
        "    def __init__(self, dataframe, root_dir, mask_root_dir, training=True):\n",
        "        self.df = dataframe\n",
        "        self.root_dir = root_dir\n",
        "        self.mask_root_dir = mask_root_dir  # ignore mask\n",
        "        self.training = training\n",
        "\n",
        "    def __len__(self):\n",
        "        return len(self.df)\n",
        "\n",
        "    def __getitem__(self, idx):\n",
        "        if torch.is_tensor(idx):\n",
        "            idx = idx.tolist()\n",
        "        \n",
        "        # Get image name\n",
        "        idx, labels = self.df.values[idx]\n",
        "        img_name = self.root_dir.format(idx)\n",
        "        \n",
        "        ## Read image\n",
        "        img0 = cv2.imread(img_name)[:,:,::-1]\n",
        "        img = preprocess_image(img0.astype(float))\n",
        "        img = np.rollaxis(img, 2, 0)\n",
        "        \n",
        "        ## Read ignore mask\n",
        "        ign_img0 = cv2.imread(self.mask_root_dir.format(idx), cv2.IMREAD_GRAYSCALE)\n",
        "        if ign_img0 is None:  # where there is no mask image available\n",
        "            ign_img0 = np.zeros((ORIG_H, ORIG_W), dtype='float32')\n",
        "\n",
        "        ign_img = np.array(ign_img0).astype('float32') / 255.\n",
        "        # ign_img = np.rollaxis(ign_img, 2, 0)\n",
        "        ######################################################\n",
        "\n",
        "        # ignore mask for CNN\n",
        "        ign_img_for_feed = preprocess_mask_image(ign_img0)\n",
        "        ign_img_for_feed = np.expand_dims(ign_img_for_feed, 0)  # h,w -> 1,h,w\n",
        "        \n",
        "        \n",
        "        # Get mask and regression maps\n",
        "        if self.training:\n",
        "            mask, regr = get_mask_and_regr(img0, labels)\n",
        "            regr = np.rollaxis(regr, 2, 0)\n",
        "        else:\n",
        "            mask, regr = 0, 0\n",
        "        \n",
        "        return [img, mask, regr, ign_img, ign_img_for_feed]"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "_kg_hide-input": true,
        "nbpresent": {
          "id": "10c1f8eb-c9a7-4338-a49a-94820b68449f"
        },
        "id": "mPdZbsXnciI9",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "train_images_dir = HOMEDIR + 'train_images/{}.jpg'\n",
        "test_images_dir = HOMEDIR + 'test_images/{}.jpg'\n",
        "train_masks_dir = HOMEDIR + 'train_masks/{}.jpg'\n",
        "test_masks_dir = HOMEDIR + 'test_masks/{}.jpg'\n",
        "\n",
        "df_train, df_dev = train_test_split(dfTrain, test_size=0.1, random_state=1042)\n",
        "df_test = dfTest\n",
        "\n",
        "## Create objects of the class type Dataset -  one for each data set\n",
        "train_dataset = CarDataset(df_train, train_images_dir, train_masks_dir)\n",
        "dev_dataset = CarDataset(df_dev, train_images_dir, train_masks_dir)\n",
        "test_dataset = CarDataset(df_test, test_images_dir, test_masks_dir)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "nbpresent": {
          "id": "bbcc6c77-29a9-4794-8fcf-c8c58ac8f935"
        },
        "id": "pgbVYdESciI_",
        "colab_type": "text"
      },
      "source": [
        "# PyTorch Model - resnet WITH mask"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "_kg_hide-input": true,
        "nbpresent": {
          "id": "a14d8f04-8d2c-42ba-88f3-969432e9afec"
        },
        "id": "GXp_NbWXciJB",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "class double_conv(nn.Module):\n",
        "    '''(conv => BN => ReLU) * 2'''\n",
        "    def __init__(self, in_ch, out_ch):\n",
        "        super(double_conv, self).__init__()\n",
        "        self.conv = nn.Sequential(\n",
        "            nn.Conv2d(in_ch, out_ch, 3, padding=1),\n",
        "            nn.BatchNorm2d(out_ch),\n",
        "            nn.ReLU(inplace=True),\n",
        "            nn.Conv2d(out_ch, out_ch, 3, padding=1),\n",
        "            nn.BatchNorm2d(out_ch),\n",
        "            nn.ReLU(inplace=True)\n",
        "        )\n",
        "\n",
        "    def forward(self, x):\n",
        "        x = self.conv(x)\n",
        "        return x\n",
        "\n",
        "class up(nn.Module):\n",
        "    def __init__(self, in_ch, out_ch, bilinear=True):\n",
        "        super(up, self).__init__()\n",
        "\n",
        "        #  would be a nice idea if the upsampling could be learned too,\n",
        "        #  but my machine do not have enough memory to handle all those weights\n",
        "        if bilinear:\n",
        "            self.up = nn.Upsample(scale_factor=2, mode='bilinear', align_corners=True)\n",
        "        else:\n",
        "            self.up = nn.ConvTranspose2d(in_ch//2, in_ch//2, 2, stride=2)\n",
        "\n",
        "        self.conv = double_conv(in_ch, out_ch)\n",
        "\n",
        "    def forward(self, x1, x2=None):\n",
        "        x1 = self.up(x1)\n",
        "        \n",
        "        # input is CHW\n",
        "        diffY = x2.size()[2] - x1.size()[2]\n",
        "        diffX = x2.size()[3] - x1.size()[3]\n",
        "\n",
        "        x1 = F.pad(x1, (diffX // 2, diffX - diffX//2,\n",
        "                        diffY // 2, diffY - diffY//2))\n",
        "        \n",
        "        # for padding issues, see \n",
        "        # https://github.com/HaiyongJiang/U-Net-Pytorch-Unstructured-Buggy/commit/0e854509c2cea854e247a9c615f175f76fbb2e3a\n",
        "        # https://github.com/xiaopeng-liao/Pytorch-UNet/commit/8ebac70e633bac59fc22bb5195e513d5832fb3bd\n",
        "        \n",
        "        if x2 is not None:\n",
        "            x = torch.cat([x2, x1], dim=1)\n",
        "        else:\n",
        "            x = x1\n",
        "        x = self.conv(x)\n",
        "        return x\n",
        "\n",
        "def get_mesh(batch_size, shape_x, shape_y):\n",
        "    mg_x, mg_y = np.meshgrid(np.linspace(0, 1, shape_y), np.linspace(0, 1, shape_x))\n",
        "    mg_x = np.tile(mg_x[None, None, :, :], [batch_size, 1, 1, 1]).astype('float32')\n",
        "    mg_y = np.tile(mg_y[None, None, :, :], [batch_size, 1, 1, 1]).astype('float32')\n",
        "    mesh = torch.cat([torch.tensor(mg_x).to(device), torch.tensor(mg_y).to(device)], 1)\n",
        "    return mesh"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "nbpresent": {
          "id": "9d436955-7847-438b-97bd-1ad901351ca6"
        },
        "id": "-g9XtE41ciJD",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "import torch.nn as nn\n",
        "import torch.nn.functional as F\n",
        "import torch.utils.model_zoo as model_zoo\n",
        "\n",
        "\n",
        "model_urls = {\n",
        "    'resnet18': 'https://download.pytorch.org/models/resnet18-5c106cde.pth',\n",
        "    'resnet34': 'https://download.pytorch.org/models/resnet34-333f7ec4.pth',\n",
        "    'resnet50': 'https://download.pytorch.org/models/resnet50-19c8e357.pth',\n",
        "    'resnet101': 'https://download.pytorch.org/models/resnet101-5d3b4d8f.pth',\n",
        "    'resnet152': 'https://download.pytorch.org/models/resnet152-b121ed2d.pth',\n",
        "}\n",
        "\n",
        "\n",
        "def conv3x3(in_planes, out_planes, stride=1):\n",
        "    \"\"\"3x3 convolution with padding\"\"\"\n",
        "    return nn.Conv2d(in_planes, out_planes, kernel_size=3, stride=stride,\n",
        "                     padding=1, bias=False)\n",
        "\n",
        "\n",
        "def conv1x1(in_planes, out_planes, stride=1):\n",
        "    \"\"\"1x1 convolution\"\"\"\n",
        "    return nn.Conv2d(in_planes, out_planes, kernel_size=1, stride=stride, bias=False)\n",
        "\n",
        "\n",
        "class BasicBlock(nn.Module):\n",
        "    expansion = 1\n",
        "\n",
        "    def __init__(self, inplanes, planes, stride=1):\n",
        "        super(BasicBlock, self).__init__()\n",
        "\n",
        "        self.conv1 = conv3x3(inplanes, planes, stride)\n",
        "        self.bn1 = nn.GroupNorm(16, planes)\n",
        "\n",
        "        self.conv2 = conv3x3(planes, planes)\n",
        "        self.bn2 = nn.GroupNorm(16, planes)\n",
        "\n",
        "        if stride != 1 or inplanes != planes:\n",
        "            self.downsample = nn.Sequential(\n",
        "                conv1x1(inplanes, planes, stride), nn.GroupNorm(16, planes))\n",
        "        else:\n",
        "            self.downsample = None\n",
        "\n",
        "\n",
        "    def forward(self, x):\n",
        "        identity = x\n",
        "\n",
        "        out = F.relu(self.bn1(self.conv1(x)), inplace=True)\n",
        "        out = self.bn2(self.conv2(out))\n",
        "\n",
        "        if self.downsample is not None:\n",
        "            identity = self.downsample(x)\n",
        "\n",
        "        out += identity\n",
        "        out = F.relu(out, inplace=True)\n",
        "\n",
        "        return out\n",
        "\n",
        "\n",
        "class Bottleneck(nn.Module):\n",
        "    expansion = 4\n",
        "\n",
        "    def __init__(self, inplanes, planes, stride=1):\n",
        "        super(Bottleneck, self).__init__()\n",
        "        self.conv1 = conv1x1(inplanes, planes)\n",
        "        self.bn1 = nn.GroupNorm(16, planes)\n",
        "        self.conv2 = conv3x3(planes, planes, stride)\n",
        "        self.bn2 = nn.GroupNorm(16, planes)\n",
        "        self.conv3 = conv1x1(planes, planes * self.expansion)\n",
        "        self.bn3 = nn.GroupNorm(16, planes * self.expansion)\n",
        "\n",
        "        if stride != 1 or inplanes != planes * self.expansion:\n",
        "            self.downsample = nn.Sequential(\n",
        "                conv1x1(inplanes, planes * self.expansion, stride), \n",
        "                nn.GroupNorm(16, planes * self.expansion))\n",
        "        else:\n",
        "            self.downsample = None\n",
        "\n",
        "    def forward(self, x):\n",
        "        identity = x\n",
        "\n",
        "        out = F.relu(self.bn1(self.conv1(x)), inplace=True)\n",
        "        out = F.relu(self.bn2(self.conv2(out)), inplace=True)\n",
        "        out = self.bn3(self.conv3(out))\n",
        " \n",
        "        if self.downsample is not None:\n",
        "            identity = self.downsample(x)\n",
        "\n",
        "        out += identity\n",
        "        out = F.relu(out)\n",
        "\n",
        "        return out\n",
        "\n",
        "\n",
        "class ResNetFeatures(nn.Module):\n",
        "\n",
        "    def __init__(self, block, layers, num_classes=1000, zero_init_residual=False, input_channels=3):\n",
        "        super(ResNetFeatures, self).__init__()\n",
        "        self.inplanes = 64\n",
        "        self.conv1 = nn.Conv2d(input_channels, 64, kernel_size=7, stride=2, padding=3,\n",
        "                               bias=False)\n",
        "        self.bn1 = nn.GroupNorm(16, 64)\n",
        "\n",
        "        self.layer1 = self._make_layer(block, 64, layers[0])\n",
        "        self.layer2 = self._make_layer(block, 128, layers[1], stride=2)\n",
        "        self.layer3 = self._make_layer(block, 256, layers[2], stride=2)\n",
        "        self.layer4 = self._make_layer(block, 512, layers[3], stride=2)\n",
        "\n",
        "\n",
        "        for m in self.modules():\n",
        "            if isinstance(m, nn.Conv2d):\n",
        "                nn.init.kaiming_normal_(m.weight, mode='fan_out', nonlinearity='relu')\n",
        "            elif isinstance(m, nn.BatchNorm2d):\n",
        "                nn.init.constant_(m.weight, 1)\n",
        "                nn.init.constant_(m.bias, 0)\n",
        "\n",
        "        # Zero-initialize the last BN in each residual branch,\n",
        "        # so that the residual branch starts with zeros, and each residual block behaves like an identity.\n",
        "        # This improves the model by 0.2~0.3% according to https://arxiv.org/abs/1706.02677\n",
        "        if zero_init_residual:\n",
        "            for m in self.modules():\n",
        "                if isinstance(m, Bottleneck):\n",
        "                    nn.init.constant_(m.bn3.weight, 0)\n",
        "                elif isinstance(m, BasicBlock):\n",
        "                    nn.init.constant_(m.bn2.weight, 0)\n",
        "\n",
        "    def _make_layer(self, block, planes, blocks, stride=1):\n",
        "\n",
        "        layers = []\n",
        "        layers.append(block(self.inplanes, planes, stride))\n",
        "        self.inplanes = planes * block.expansion\n",
        "        for _ in range(1, blocks):\n",
        "            layers.append(block(self.inplanes, planes))\n",
        "\n",
        "        return nn.Sequential(*layers)\n",
        "\n",
        "\n",
        "    def forward(self, x):\n",
        "        conv1 = F.relu(self.bn1(self.conv1(x)), inplace=True)\n",
        "        conv1 = F.max_pool2d(conv1, 3, stride=2, padding=1)\n",
        "\n",
        "        feats4 = self.layer1(conv1)\n",
        "        feats8 = self.layer2(feats4)\n",
        "        feats16 = self.layer3(feats8)\n",
        "        feats32 = self.layer4(feats16)\n",
        "\n",
        "        return feats8, feats16, feats32\n",
        "\n",
        "\n",
        "\n",
        "def resnet18(pretrained=False, **kwargs):\n",
        "    \"\"\"Constructs a ResNet-18 model.\n",
        "    Args:\n",
        "        pretrained (bool): If True, returns a model pre-trained on ImageNet\n",
        "    \"\"\"\n",
        "    model = ResNetFeatures(BasicBlock, [2, 2, 2, 2], **kwargs)\n",
        "    if pretrained:\n",
        "        _load_pretrained(model, model_zoo.load_url(model_urls['resnet18']))\n",
        "    return model\n",
        "\n",
        "\n",
        "\n",
        "def resnet34(pretrained=False, **kwargs):\n",
        "    \"\"\"Constructs a ResNet-34 model.\n",
        "    Args:\n",
        "        pretrained (bool): If True, returns a model pre-trained on ImageNet\n",
        "    \"\"\"\n",
        "    model = ResNetFeatures(BasicBlock, [3, 4, 6, 3], **kwargs)\n",
        "    if pretrained:\n",
        "        _load_pretrained(model, model_zoo.load_url(model_urls['resnet34']))\n",
        "    return model\n",
        "\n",
        "\n",
        "def _load_pretrained(model, pretrained):\n",
        "    model_dict = model.state_dict()\n",
        "    pretrained = {k : v for k, v in pretrained.items() if k in model_dict}\n",
        "    model_dict.update(pretrained)\n",
        "    model.load_state_dict(model_dict)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "nbpresent": {
          "id": "785e5bb6-8333-42bc-9275-e2bd63dff890"
        },
        "id": "iObQE-UeciJG",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "class CentResnet(nn.Module):\n",
        "    '''Mixture of previous classes'''\n",
        "    def __init__(self, n_classes):\n",
        "        super(CentResnet, self).__init__()\n",
        "        self.base_model = resnet18(pretrained=False, input_channels=6)\n",
        "        \n",
        "        # Lateral layers convert resnet outputs to a common feature size\n",
        "        self.lat8 = nn.Conv2d(128, 256, 1)\n",
        "        self.lat16 = nn.Conv2d(256, 256, 1)\n",
        "        self.lat32 = nn.Conv2d(512, 256, 1)\n",
        "        self.bn8 = nn.GroupNorm(16, 256)\n",
        "        self.bn16 = nn.GroupNorm(16, 256)\n",
        "        self.bn32 = nn.GroupNorm(16, 256)\n",
        "\n",
        "        if USEMASK:\n",
        "            self.conv0 = double_conv(5 + 1, 64)\n",
        "        else:\n",
        "            self.conv0 = double_conv(5, 64)\n",
        "            \n",
        "        self.conv1 = double_conv(64, 128)\n",
        "        self.conv2 = double_conv(128, 512)\n",
        "        self.conv3 = double_conv(512, 1024)\n",
        "        \n",
        "        self.mp = nn.MaxPool2d(2)\n",
        "        \n",
        "        self.up1 = up(1282 , 512) #+ 1024\n",
        "        self.up2 = up(512 + 512, 256)\n",
        "        self.outc = nn.Conv2d(256, n_classes, 1)\n",
        "        \n",
        "    \n",
        "    def forward(self, x):\n",
        "        batch_size = x.shape[0]\n",
        "        mesh1 = get_mesh(batch_size, x.shape[2], x.shape[3])\n",
        "        x0 = torch.cat([x, mesh1], 1)\n",
        "        x1 = self.mp(self.conv0(x0))\n",
        "        x2 = self.mp(self.conv1(x1))\n",
        "        x3 = self.mp(self.conv2(x2))\n",
        "        x4 = self.mp(self.conv3(x3))\n",
        "        \n",
        "        #feats = self.base_model.extract_features(x)\n",
        "                # Run frontend network\n",
        "        if USEMASK:\n",
        "            ## feats8, feats16, feats32 = self.base_model(x[:,0:3])  ## use first 3 channel. this may not be proper way\n",
        "            feats8, feats16, feats32 = self.base_model(x0)  ## C=6 : rgb(3)+mask(1)+mesh(2)\n",
        "        else:\n",
        "            feats8, feats16, feats32 = self.base_model(x)\n",
        "\n",
        "        lat8 = F.relu(self.bn8(self.lat8(feats8)))\n",
        "        lat16 = F.relu(self.bn16(self.lat16(feats16)))\n",
        "        lat32 = F.relu(self.bn32(self.lat32(feats32)))\n",
        "        \n",
        "        # Add positional info\n",
        "        mesh2 = get_mesh(batch_size, lat32.shape[2], lat32.shape[3])\n",
        "        feats = torch.cat([lat32, mesh2], 1)\n",
        "        #print(feats.shape)\n",
        "        #print (x4.shape)\n",
        "        x = self.up1(feats, x4)\n",
        "        x = self.up2(x, x3)\n",
        "        x = self.outc(x)\n",
        "        return x"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "nbpresent": {
          "id": "4a74a8a2-7c49-455a-a66e-9951011f89f3"
        },
        "id": "WaULxKXDciJJ",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "def infer_image(img, ign_mask_for_feed):  # shape:[B,C,H,W]\n",
        "    return model( torch.from_numpy( np.concatenate((img, ign_mask_for_feed),axis=1) ).to(device) )"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "T3uWroDGciJM",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "def sigmoid(x):\n",
        "    return 1 / (1 + np.exp(-x))\n",
        "\n",
        "######## def postprocess_heatmap(logits, thresh=0.60):  # V6+ignMask, LB0.062\n",
        "######## def postprocess_heatmap(logits, thresh=0.50):  # V6+ignMask, LB0.066\n",
        "######## def postprocess_heatmap(logits, thresh=0.40):  # V6+ignMask, LB0.065\n",
        "######## def postprocess_heatmap(logits, thresh=0.30):  # V6+ignMask, LB0.063\n",
        "\n",
        "### def postprocess_heatmap(logits, thresh=0.4):\n",
        "def postprocess_heatmap(logits, thresh=0.45):\n",
        "    prob = sigmoid(logits)\n",
        "    mp2d = torch.nn.MaxPool2d(3, stride=1, padding=1, dilation=1, return_indices=False, ceil_mode=False)\n",
        "    out = mp2d( torch.Tensor([[prob]]) ).numpy()[0][0]\n",
        "    return (prob == out) & (prob > thresh)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "nbpresent": {
          "id": "b6792e19-dd80-4de3-b4b4-10d135281a28"
        },
        "id": "Z7k4n4UPciJO",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "DISTANCE_THRESH_CLEAR = 2\n"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "IeAgVJqrciJR",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "def clear_duplicates(coords):\n",
        "    for c1 in coords:\n",
        "        xyz1 = np.array([c1['x'], c1['y'], c1['z']])\n",
        "        for c2 in coords:\n",
        "            xyz2 = np.array([c2['x'], c2['y'], c2['z']])\n",
        "            distance = np.sqrt(((xyz1 - xyz2)**2).sum())\n",
        "            if distance < DISTANCE_THRESH_CLEAR:\n",
        "                if c1['confidence'] < c2['confidence']:\n",
        "                    c1['confidence'] = -1\n",
        "    return [c for c in coords if c['confidence'] > 0]"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "iV1V500rciJU",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "def extract_coords(prediction, ign_mask):\n",
        "    assert ign_mask.shape[0] == ORIG_H   #\n",
        "    logits = prediction[0]\n",
        "    regr_output = prediction[1:]\n",
        "    points_mat = postprocess_heatmap(logits) \n",
        "    points = np.argwhere( points_mat > 0 )\n",
        "    \n",
        "    col_names = sorted(REGR_TARGETS)  # vdiff,udiff,z,yaw,pitch_sin,pitch_cos,roll\n",
        "    coords = []\n",
        "    for r, c in points:           \n",
        "        regr_dict = dict(zip(col_names, regr_output[:, r, c]))\n",
        "        \n",
        "        # use heatmap-peak (V,U) position\n",
        "        regr_backed = _regr_back(regr_dict, r, c)\n",
        "        \n",
        "        _U, _V = XYZ2UV(regr_backed[\"x\"], regr_backed[\"y\"], regr_backed[\"z\"])\n",
        "        _U, _V = int(_U), int(_V)\n",
        "        if _V>=0 and _V<ORIG_H and _U>=0 and _U<ORIG_W and ign_mask[_V,_U] > 0.5:  # floor(u), floor(v)\n",
        "            # print(\"point is in ignore_mask\")\n",
        "            continue\n",
        "\n",
        "        coords.append(regr_backed)\n",
        "        coords[-1]['confidence'] = 1 / (1 + np.exp(-logits[r, c]))\n",
        "\n",
        "        coords = clear_duplicates(coords)\n",
        "    return coords"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "EVV4uafjciJX",
        "colab_type": "text"
      },
      "source": [
        "# Load each model and find prediction time"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "y9RcnbYSciJY",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 102
        },
        "outputId": "309bb5ee-ba06-446d-eb47-cb3318b2a54f"
      },
      "source": [
        "import datetime\n",
        "time_start = datetime.datetime.now()\n",
        "print(time_start)\n",
        "time_end = datetime.datetime.now()\n",
        "print(time_end)\n",
        "\n",
        "time_delta = time_end - time_start\n",
        "\n",
        "print(time_delta)\n",
        "print(type(time_delta))\n",
        "\n",
        "time_avg = time_delta / 2\n",
        "print(time_avg)"
      ],
      "execution_count": 31,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "2020-03-09 18:23:05.459245\n",
            "2020-03-09 18:23:05.460032\n",
            "0:00:00.000787\n",
            "<class 'datetime.timedelta'>\n",
            "0:00:00.000394\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "ZhlDz41RciJb",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 34
        },
        "outputId": "1a1fc93e-9585-4553-a138-ea0ceb084186"
      },
      "source": [
        "import datetime\n",
        "\n",
        "import gc\n",
        "torch.cuda.empty_cache()\n",
        "gc.collect()"
      ],
      "execution_count": 32,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "0"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 32
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "VhPkGIIYciJf",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 105
        },
        "outputId": "d537f0b8-ca8c-45e8-caa0-8e3003983c5a"
      },
      "source": [
        "print(f\"{os.listdir(model_path_dir)}\\n\\n\")\n",
        "dict_models = {}\n",
        "for eachmodel in os.listdir(model_path_dir):\n",
        "    dict_models[eachmodel] = model_path_dir + eachmodel\n",
        "\n",
        "print(f\"{dict_models}\")"
      ],
      "execution_count": 33,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "['model_kagg_rohit-center-resnet-1_ep4_loss_bct_mask.pth', 'model_kagg-rohit-center-resnet-1_ep4-loss-focal-mask.pth', 'model_kagg_manmeet-center-resnet-1_ep10_loss_focal_mask.pth', 'model_kagg_manmeet_centernet_resnet_ep_6_loss_focal_mask_20200301_0200.pth']\n",
            "\n",
            "\n",
            "{'model_kagg_rohit-center-resnet-1_ep4_loss_bct_mask.pth': '/content/drive/My Drive/baidu/Models/Cent-Resnet18_WITHmask/model_kagg_rohit-center-resnet-1_ep4_loss_bct_mask.pth', 'model_kagg-rohit-center-resnet-1_ep4-loss-focal-mask.pth': '/content/drive/My Drive/baidu/Models/Cent-Resnet18_WITHmask/model_kagg-rohit-center-resnet-1_ep4-loss-focal-mask.pth', 'model_kagg_manmeet-center-resnet-1_ep10_loss_focal_mask.pth': '/content/drive/My Drive/baidu/Models/Cent-Resnet18_WITHmask/model_kagg_manmeet-center-resnet-1_ep10_loss_focal_mask.pth', 'model_kagg_manmeet_centernet_resnet_ep_6_loss_focal_mask_20200301_0200.pth': '/content/drive/My Drive/baidu/Models/Cent-Resnet18_WITHmask/model_kagg_manmeet_centernet_resnet_ep_6_loss_focal_mask_20200301_0200.pth'}\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "scrolled": true,
        "id": "IwHAIV6GciJh",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 34
        },
        "outputId": "1a445fdf-067d-4529-8422-21b83707eb36"
      },
      "source": [
        "dict_models.pop('model_kagg_rohit-center-resnet-1_ep4_loss_bct_mask.pth')\n",
        "dict_models.pop('model_kagg_manmeet-center-resnet-1_ep10_loss_focal_mask.pth')\n",
        "dict_models.pop('model_kagg_manmeet_centernet_resnet_ep_6_loss_focal_mask_20200301_0200.pth')"
      ],
      "execution_count": 34,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "'/content/drive/My Drive/baidu/Models/Cent-Resnet18_WITHmask/model_kagg_manmeet_centernet_resnet_ep_6_loss_focal_mask_20200301_0200.pth'"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 34
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "6PR6NVPUciJk",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "## running only 3rd model = 'model_kagg-rohit-center-resnet-1_ep4-loss-focal-mask.pth'"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "wtZTlZTYciJo",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 34
        },
        "outputId": "3537791b-d149-45d0-f581-1098ede38dcd"
      },
      "source": [
        "print(f\"{dict_models}\")"
      ],
      "execution_count": 36,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "{'model_kagg-rohit-center-resnet-1_ep4-loss-focal-mask.pth': '/content/drive/My Drive/baidu/Models/Cent-Resnet18_WITHmask/model_kagg-rohit-center-resnet-1_ep4-loss-focal-mask.pth'}\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "-m3RFJCzciJr",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 34
        },
        "outputId": "99f62be2-819b-4120-8ff9-bf33e034e9bc"
      },
      "source": [
        "len(df_dev)"
      ],
      "execution_count": 37,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "426"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 37
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "scrolled": false,
        "id": "dPoBa9v5ciJu",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 323
        },
        "outputId": "e44889a5-fda5-4d27-d76b-bbad6040c353"
      },
      "source": [
        "print(f\"USEMASK set as = {USEMASK}\\n\\n\")\n",
        "\n",
        "#NUMBER_IMAGES_TO_PREDICT = 2  # for testing\n",
        "NUMBER_IMAGES_TO_PREDICT = len(df_dev)  # uncomment to use full 426 values of dev\n",
        "\n",
        "print(f\"NUMBER_IMAGES_TO_PREDICT = {NUMBER_IMAGES_TO_PREDICT}\\n\")\n",
        "\n",
        "time_start = None\n",
        "time_end = None\n",
        "time_delta = None\n",
        "time_avg = None\n",
        "\n",
        "model_idx=1\n",
        "\n",
        "for each_model_name, each_model_path in dict_models.items():\n",
        "    #\n",
        "    print(f\"\\n--------- For model number #{model_idx} ---------\\n\")\n",
        "    #\n",
        "    torch.cuda.empty_cache()\n",
        "    gc.collect()\n",
        "    model = None\n",
        "    saved_model = None\n",
        "    time_start = None\n",
        "    time_end = None\n",
        "    time_avg = None\n",
        "    #\n",
        "    ## Load the saved model\n",
        "    saved_model = CentResnet(8).to(device)\n",
        "    saved_model.load_state_dict(torch.load(each_model_path))\n",
        "    model = saved_model\n",
        "    #saved_model.eval()\n",
        "    #type(model)\n",
        "    #\n",
        "    time_start = datetime.datetime.now()\n",
        "    for idx in range(NUMBER_IMAGES_TO_PREDICT):\n",
        "        img, heatmap, regr, ign_mask, ign_mask_for_feed = dev_dataset[idx]\n",
        "        mask = (heatmap >= 1).astype(float)\n",
        "    \n",
        "        if USEMASK:\n",
        "            output = infer_image(img[None], ign_mask_for_feed[None])\n",
        "            output = output.data.cpu().numpy()\n",
        "        else:\n",
        "            output = model(torch.tensor(img[None]).to(device)).data.cpu().numpy()\n",
        "    #\n",
        "    time_end = datetime.datetime.now()\n",
        "    time_delta = time_end - time_start\n",
        "    time_avg = time_delta / NUMBER_IMAGES_TO_PREDICT\n",
        "    \n",
        "    print(f\"model = {each_model_name}\\nTotal time_delta = {time_delta}\\naverage time per image = {time_avg}\\n\\npicked from location = {each_model_path}\")\n",
        "    \n",
        "    model_idx += 1\n",
        "\n",
        "print(f\"\\n\\nDONE\\n\\n\")"
      ],
      "execution_count": 38,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "USEMASK set as = True\n",
            "\n",
            "\n",
            "NUMBER_IMAGES_TO_PREDICT = 426\n",
            "\n",
            "\n",
            "--------- For model number #1 ---------\n",
            "\n",
            "model = model_kagg-rohit-center-resnet-1_ep4-loss-focal-mask.pth\n",
            "Total time_delta = 0:14:26.332962\n",
            "average time per image = 0:00:02.033645\n",
            "\n",
            "picked from location = /content/drive/My Drive/baidu/Models/Cent-Resnet18_WITHmask/model_kagg-rohit-center-resnet-1_ep4-loss-focal-mask.pth\n",
            "\n",
            "\n",
            "DONE\n",
            "\n",
            "\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "3B-JzY6fciJx",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        ""
      ],
      "execution_count": 0,
      "outputs": []
    }
  ]
}