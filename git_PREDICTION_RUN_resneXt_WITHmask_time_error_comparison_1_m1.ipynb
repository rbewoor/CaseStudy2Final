{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "-mLA643s2d7P"
   },
   "source": [
    "# PREDICTION comparison using the saved models:\n",
    "\n",
    "## resnet , WITH mask\n",
    "\n",
    "## running only 1st model = 'model_manmeet_resneXt_ep4_focal_Mask_20200310_0600.pth'\n",
    "\n",
    "### 1) Time\n",
    "\n",
    "### 2) MSE for the outputs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "_cell_guid": "b1076dfc-b9ad-4769-8c92-a6c4dae69d19",
    "_kg_hide-input": true,
    "_kg_hide-output": true,
    "_uuid": "8f2839f25d086af736a60e9eeb907d3b93b6e0e5",
    "colab": {},
    "colab_type": "code",
    "id": "Ax4bS0Pz2d7R",
    "nbpresent": {
     "id": "6a63605e-0508-479a-94d0-edc7195a51a2"
    }
   },
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "import cv2\n",
    "from tqdm import tqdm_notebook as tqdm\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "from functools import reduce\n",
    "import os\n",
    "from sklearn.model_selection import train_test_split\n",
    "from scipy.optimize import minimize\n",
    "from tqdm.auto import tqdm as tq\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "import torch.optim as optim\n",
    "from torch.optim import lr_scheduler\n",
    "from torch.utils.data import Dataset, DataLoader\n",
    "from torchvision import models\n",
    "from torchvision import transforms, utils\n",
    "\n",
    "import albumentations as alb"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 34
    },
    "colab_type": "code",
    "id": "6Kv8jjPo2d7W",
    "outputId": "e1672c20-402b-4b58-f83d-8f933dee31c3"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "cuda\n"
     ]
    }
   ],
   "source": [
    "# Gets the GPU if there is one, otherwise the cpu\n",
    "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "#device = torch.device(\"cpu\")\n",
    "print(device)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "RdvMPjSd2d7a"
   },
   "outputs": [],
   "source": [
    "USEMASK = True\n",
    "\n",
    "KAGGLE = False\n",
    "COLAB = True\n",
    "\n",
    "# SPECIFY THE MODEL PATH BELOW BEFORE RUNNING"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "lropCEJI2d7e"
   },
   "outputs": [],
   "source": [
    "## KAGGLE and COLAB flags should not be True at same time - fail if this is case\n",
    "assert not (KAGGLE and COLAB), \"both KAGGLE and COLAB runs cannot be true\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 338
    },
    "colab_type": "code",
    "id": "rQrDphUp2d7i",
    "outputId": "8edf5ac5-7e2f-43c7-bd76-909c23de67cb"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Go to this URL in a browser: https://accounts.google.com/o/oauth2/auth?client_id=947318989803-6bn6qk8qdgf4n4g3pfee6491hc0brc4i.apps.googleusercontent.com&redirect_uri=urn%3aietf%3awg%3aoauth%3a2.0%3aoob&response_type=code&scope=email%20https%3a%2f%2fwww.googleapis.com%2fauth%2fdocs.test%20https%3a%2f%2fwww.googleapis.com%2fauth%2fdrive%20https%3a%2f%2fwww.googleapis.com%2fauth%2fdrive.photos.readonly%20https%3a%2f%2fwww.googleapis.com%2fauth%2fpeopleapi.readonly\n",
      "\n",
      "Enter your authorization code:\n",
      "··········\n",
      "Mounted at /content/drive\n",
      "COLAB\n",
      "laptop, not CLOUD\n",
      "\n",
      "HOMEDIR =\n",
      "/content/drive/My Drive/baidu/pku-autonomous-driving/\n",
      "\n",
      "OUTDIR=\n",
      "./content/drive/My Drive/baidu/pku-autonomous-driving/outputROHIT/\n",
      "\n",
      "model_path_dir =\n",
      "/content/drive/My Drive/baidu/Models/Cent-ResneXt_WITHmask/\n",
      "\n"
     ]
    }
   ],
   "source": [
    "if KAGGLE and not COLAB:          #  kaggle run\n",
    "    HOMEDIR = r'../input/pku-autonomous-driving/'\n",
    "    OUTDIR = r'./'\n",
    "    model_path_dir = None\n",
    "    \n",
    "elif COLAB and not KAGGLE:         # google colab\n",
    "    from google.colab import drive\n",
    "    drive.mount('/content/drive')\n",
    "    #\n",
    "    HOMEDIR = r'/content/drive/My Drive/baidu/pku-autonomous-driving/'\n",
    "    OUTDIR = r'./content/drive/My Drive/baidu/pku-autonomous-driving/outputROHIT/'\n",
    "    model_path_dir = r'/content/drive/My Drive/baidu/Models/Cent-ResneXt_WITHmask/'\n",
    "\n",
    "elif not KAGGLE and not COLAB:\n",
    "    HOMEDIR = r\"/media/rohit/DATA/EverythingD/01SRH-BDBA Acads/CaseStudy2/Kaggle-PekingAutonomousDriving/pku-autonomous-driving/\"\n",
    "    OUTDIR = r\"/media/rohit/DATA/EverythingD/01SRH-BDBA Acads/CaseStudy2/Kaggle-PekingAutonomousDriving/pku-autonomous-driving/output/\"\n",
    "    model_path_dir = r'/home/rohit/SRH/CaseStudy2/Models/Cent-ResneXt_WITHmask/'\n",
    "\n",
    "#\n",
    "if COLAB: print(f\"COLAB\")\n",
    "if KAGGLE: print(f\"KAGGLE\")\n",
    "if not (KAGGLE and COLAB): print(f\"laptop, not CLOUD\\n\")\n",
    "else: print(f\"on CLOUD\\n\")\n",
    "\n",
    "print(f\"HOMEDIR =\\n{HOMEDIR}\\n\\nOUTDIR=\\n{OUTDIR}\\n\\nmodel_path_dir =\\n{model_path_dir}\\n\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "mc4yrJJe2d7m"
   },
   "outputs": [],
   "source": [
    "assert model_path_dir is not None, \"model path directory is not specified\""
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "X0nfKzmL2d7q",
    "nbpresent": {
     "id": "0024d699-064c-4fb3-a3c2-04bb8c3c65ea"
    }
   },
   "source": [
    "# Basic info loading\n",
    "\n",
    "## data, camera matrix specification\n",
    "\n",
    "\n",
    "## train.csv   has   ImageId, PredictionString\n",
    "\n",
    "## Prediction String    model# yaw pitch roll x y z  and the same for multiple cars"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "_cell_guid": "79c7e3d0-c299-4dcb-8224-4455121ee9b0",
    "_kg_hide-input": true,
    "_uuid": "d629ff2d2480ee46fbb7e2d37f6b5fab8052498a",
    "colab": {},
    "colab_type": "code",
    "id": "LgHGYic52d7r",
    "nbpresent": {
     "id": "63ca552d-09e4-4632-bda9-20e51805d3ee"
    }
   },
   "outputs": [],
   "source": [
    "dfTrain = pd.read_csv(HOMEDIR + 'train.csv')\n",
    "dfTest = pd.read_csv(HOMEDIR + 'sample_submission.csv')\n",
    "\n",
    "# From camera.zip\n",
    "camera_matrix = np.array([[2304.5479, 0,  1686.2379],\n",
    "                          [0, 2305.8757, 1354.9849],\n",
    "                          [0, 0, 1]], dtype=np.float32)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 197
    },
    "colab_type": "code",
    "id": "lcO-edvm2d7u",
    "outputId": "91ad90fc-85e5-406b-94b5-f4cad6b531ed"
   },
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>ImageId</th>\n",
       "      <th>PredictionString</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>ID_8a6e65317</td>\n",
       "      <td>16 0.254839 -2.57534 -3.10256 7.96539 3.20066 ...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>ID_337ddc495</td>\n",
       "      <td>66 0.163988 0.192169 -3.12112 -3.17424 6.55331...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>ID_a381bf4d0</td>\n",
       "      <td>43 0.162877 0.00519276 -3.02676 2.1876 3.53427...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>ID_7c4a3e0aa</td>\n",
       "      <td>43 0.126957 -3.04442 -3.10883 -14.738 24.6389 ...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>ID_8b510fad6</td>\n",
       "      <td>37 0.16017 0.00862796 -3.0887 -3.04548 3.4977 ...</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "        ImageId                                   PredictionString\n",
       "0  ID_8a6e65317  16 0.254839 -2.57534 -3.10256 7.96539 3.20066 ...\n",
       "1  ID_337ddc495  66 0.163988 0.192169 -3.12112 -3.17424 6.55331...\n",
       "2  ID_a381bf4d0  43 0.162877 0.00519276 -3.02676 2.1876 3.53427...\n",
       "3  ID_7c4a3e0aa  43 0.126957 -3.04442 -3.10883 -14.738 24.6389 ...\n",
       "4  ID_8b510fad6  37 0.16017 0.00862796 -3.0887 -3.04548 3.4977 ..."
      ]
     },
     "execution_count": 8,
     "metadata": {
      "tags": []
     },
     "output_type": "execute_result"
    }
   ],
   "source": [
    "dfTrain.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 52
    },
    "colab_type": "code",
    "id": "9C0Umjei2d7y",
    "nbpresent": {
     "id": "10342326-4cfb-48ed-9384-ad869c9cecba"
    },
    "outputId": "696a74c8-e691-4469-fdb0-e4fea0116318"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "full df len = 4262\n",
      "post removal of bad entries len = 4257\n"
     ]
    }
   ],
   "source": [
    "bad_images_list = [\"ID_1a5a10365\",\"ID_4d238ae90\",\"ID_408f58e9f\",\"ID_bb1d991f6\",\"ID_c44983aeb\"]\n",
    "for bad_id in bad_images_list:\n",
    "    #plt.imshow( cv2.imread(HOMEDIR + 'train_images/' + bad_id + '.jpg')[:,:,::-1] )\n",
    "    #plt.show()\n",
    "    pass\n",
    "print(f\"full df len = {len(dfTrain)}\")\n",
    "drop_dfTrain = dfTrain.set_index(\"ImageId\").drop( index=bad_images_list )\n",
    "dfTrain = drop_dfTrain.reset_index()\n",
    "print(f\"post removal of bad entries len = {len(dfTrain)}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "3b483ysU2d73"
   },
   "source": [
    "# Set swtich for very small data run"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "Gx9hPqCi2d74"
   },
   "outputs": [],
   "source": [
    "TEST_SWITCH_ON = False"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "xgy7vc5c2d78"
   },
   "outputs": [],
   "source": [
    "if TEST_SWITCH_ON:\n",
    "    dfTrain = dfTrain[:20]\n",
    "    dfTest  = dfTest[:6]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "ay4il84Q2d7_",
    "nbpresent": {
     "id": "899ba2fa-a9a6-4b4a-b3d8-8a1fb11d037b"
    }
   },
   "outputs": [],
   "source": [
    "def str2coords(ps, names=['id', 'yaw', 'pitch', 'roll', 'x', 'y', 'z']):\n",
    "# from the prediction string entries, create a dict of each car data\n",
    "    coords = []\n",
    "    for ps_entry in np.array(ps.split()).reshape([-1, 7]):\n",
    "        dictval = dict(zip(names, ps_entry.astype('float')))\n",
    "        coords.append(dictval)\n",
    "        if 'id' in coords[-1]:\n",
    "            coords[-1]['id'] = int(coords[-1]['id'])\n",
    "    return coords"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "hHY2Kdin2d8C",
    "nbpresent": {
     "id": "63a68ff2-3fd7-4c58-aa77-c97b5a309926"
    }
   },
   "source": [
    "# 2D Visualization"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "VhSHeM9b2d8D"
   },
   "outputs": [],
   "source": [
    "def rotate(x, angle):\n",
    "    x = x + angle\n",
    "    x = x - (x + np.pi) // (2 * np.pi) * 2 * np.pi\n",
    "    return x"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "xzzFgjpK2d8G"
   },
   "outputs": [],
   "source": [
    "def read_image(path):\n",
    "    img = cv2.imread(path)\n",
    "    #img = np.array(img[:, :, ::-1]) # alternative way to convert BGR to RGB\n",
    "    img = cv2.cvtColor(img, cv2.COLOR_BGR2RGB) \n",
    "    return img"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "EKmUGtaa2d8K",
    "nbpresent": {
     "id": "64874f90-1066-4d31-91a8-76d74e9024ac"
    }
   },
   "outputs": [],
   "source": [
    "def convert_to_img_coords(ps):\n",
    "# convert the camera coords x,y,z to the image coords\n",
    "    coords = str2coords(ps)\n",
    "    x_list = [c['x'] for c in coords]\n",
    "    y_list = [c['y'] for c in coords]\n",
    "    z_list = [c['z'] for c in coords]\n",
    "    P = np.array(list(zip(x_list, y_list, z_list))).T\n",
    "    img_p = np.dot(camera_matrix, P).T\n",
    "    img_p[:, 0] /= img_p[:, 2]\n",
    "    img_p[:, 1] /= img_p[:, 2]\n",
    "    img_x_list = img_p[:, 0]\n",
    "    img_y_list = img_p[:, 1]\n",
    "    img_z_list = img_p[:, 2]\n",
    "    return img_x_list, img_y_list"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "EjxezRmK2d8O",
    "nbpresent": {
     "id": "c3ed48e9-2bea-479e-8e38-f8743b00c6ec"
    }
   },
   "source": [
    "# 3D Visualization"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "_kg_hide-input": true,
    "colab": {},
    "colab_type": "code",
    "id": "y5xj4PUY2d8P",
    "nbpresent": {
     "id": "0620bdfe-a8cc-4f28-a488-faf81a748072"
    }
   },
   "outputs": [],
   "source": [
    "from math import sin, cos\n",
    "\n",
    "# convert euler angle to rotation matrix\n",
    "def euler_to_Rot(yaw, pitch, roll):\n",
    "    Y = np.array([[cos(yaw), 0, sin(yaw)],\n",
    "                  [0, 1, 0],\n",
    "                  [-sin(yaw), 0, cos(yaw)]])\n",
    "    P = np.array([[1, 0, 0],\n",
    "                  [0, cos(pitch), -sin(pitch)],\n",
    "                  [0, sin(pitch), cos(pitch)]])\n",
    "    R = np.array([[cos(roll), -sin(roll), 0],\n",
    "                  [sin(roll), cos(roll), 0],\n",
    "                  [0, 0, 1]])\n",
    "    return np.dot(Y, np.dot(P, R))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "_kg_hide-input": true,
    "colab": {},
    "colab_type": "code",
    "id": "0QIWAgi32d8U",
    "nbpresent": {
     "id": "4b22780a-18c3-4d1e-92e2-57f0f41c385e"
    }
   },
   "outputs": [],
   "source": [
    "def draw_line(image, points):\n",
    "    color = (255, 0, 0)\n",
    "    cv2.line(image, tuple(points[0][:2]), tuple(points[3][:2]), color, 16)\n",
    "    cv2.line(image, tuple(points[0][:2]), tuple(points[1][:2]), color, 16)\n",
    "    cv2.line(image, tuple(points[1][:2]), tuple(points[2][:2]), color, 16)\n",
    "    cv2.line(image, tuple(points[2][:2]), tuple(points[3][:2]), color, 16)\n",
    "    return image\n",
    "\n",
    "\n",
    "def draw_points(image, points):\n",
    "    for (p_x, p_y, p_z) in points:\n",
    "        cv2.circle(image, (p_x, p_y), int(1000 / p_z), (0, 255, 0), -1)\n",
    "#         if p_x > image.shape[1] or p_y > image.shape[0]:\n",
    "#             print('Point', p_x, p_y, 'is out of image with shape', image.shape)\n",
    "    return image"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "YV3jBhGE2d8X"
   },
   "source": [
    "# Average ratios of the spanX, spanY and spanZ  by groups decided by me\n",
    "\n",
    "GroupNo based on Z / X value bin.\n",
    "GroupNo     Xspan       Yspan       Zspan       Yspan/Xspan     Zspan/Xspan\n",
    "1           1.96        1.58        3.78\t\t0.80            1.92\n",
    "2           2.08        1.65        4.49        0.80            2.15\n",
    "3           2.03        1.57        4.86        0.78            2.38\n",
    "\n",
    "average of all these\n",
    "GroupNo     Xspan       Yspan       Zspan       Yspan/Xspan     Zspan/Xspan\n",
    "            1.00        0.79        2.15        0.79            2.15"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "QQS4rBOb2d8Y",
    "nbpresent": {
     "id": "0bf0b717-12ce-42fa-855b-eb00090d9571"
    }
   },
   "outputs": [],
   "source": [
    "def visualize(img, coords):\n",
    "    ## want to visualize \n",
    "    x_l = 1.00\n",
    "    y_l = 0.79\n",
    "    z_l = 2.15\n",
    "    \n",
    "    img = img.copy()\n",
    "    for point in coords:\n",
    "        # Get values\n",
    "        x, y, z = point['x'], point['y'], point['z']\n",
    "        ## the yaw and pitch is interchanged in the data provided\n",
    "        yaw, pitch, roll = -point['pitch'], -point['yaw'], -point['roll']\n",
    "        # Math\n",
    "        Rt = np.eye(4)\n",
    "        t = np.array([x, y, z])\n",
    "        Rt[:3, 3] = t\n",
    "        Rt[:3, :3] = euler_to_Rot(yaw, pitch, roll).T\n",
    "        Rt = Rt[:3, :]\n",
    "        P = np.array([[x_l, -y_l, -z_l, 1],\n",
    "                      [x_l, -y_l, z_l, 1],\n",
    "                      [-x_l, -y_l, z_l, 1],\n",
    "                      [-x_l, -y_l, -z_l, 1],\n",
    "                      [0, 0, 0, 1]]).T\n",
    "        img_cor_points = np.dot(camera_matrix, np.dot(Rt, P))\n",
    "        img_cor_points = img_cor_points.T\n",
    "        img_cor_points[:, 0] /= img_cor_points[:, 2]\n",
    "        img_cor_points[:, 1] /= img_cor_points[:, 2]\n",
    "        img_cor_points = img_cor_points.astype(int)\n",
    "        # Drawing\n",
    "        img = draw_line(img, img_cor_points)\n",
    "        img = draw_points(img, img_cor_points[-1:])\n",
    "    \n",
    "    return img"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "DqeJqXNo2d8b",
    "nbpresent": {
     "id": "6f1121d8-0353-453f-8a99-b1230811b173"
    }
   },
   "source": [
    "# Steps to preprocess input image"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "TOh3jGOR2d8c",
    "nbpresent": {
     "id": "04405c31-3249-40d8-963c-0a1006947fbb"
    }
   },
   "outputs": [],
   "source": [
    "# Original dimensions of the Train and Test images\n",
    "ORIG_W = 3384\n",
    "ORIG_H = 2710\n",
    "\n",
    "# The dimensions we want to use for processing: keeping ratio of width:height = 1:4\n",
    "IMG_WIDTH = 2048\n",
    "IMG_HEIGHT = 512\n",
    "MARGIN_W = ORIG_W // 4  # 846\n",
    "\n",
    "MODEL_SCALE = 8  # mask shrink rate\n",
    "\n",
    "FX, FY = 2304.5479,  2305.8757\n",
    "CX, CY = 1686.2379, 1354.9849\n",
    "def XYZ2UV(x,y,z):\n",
    "    u = FX * x / z + CX\n",
    "    v = FY * y / z + CY\n",
    "    return u,v\n",
    "def UVZ2XY(u,v,z):\n",
    "    x = z * (u - CX) / FX\n",
    "    y = z * (v - CY) / FY\n",
    "    return x,y\n",
    "\n",
    "#\n",
    "# u is horizontal dimension and v is vertical dimension\n",
    "#\n",
    "def VU2maskVU(v,u):  \n",
    "    mask_V = (v - ORIG_H // 2) * IMG_HEIGHT / (ORIG_H // 2) / MODEL_SCALE\n",
    "    mask_U = (u + MARGIN_W) * IMG_WIDTH  / (ORIG_W + 2*MARGIN_W) / MODEL_SCALE\n",
    "    return mask_V, mask_U\n",
    "def maskVU2VU(mask_v_float, mask_u_float):\n",
    "    v = ORIG_H // 2 + mask_v_float * MODEL_SCALE / IMG_HEIGHT * (ORIG_H // 2)\n",
    "    u = mask_u_float * MODEL_SCALE * (ORIG_W + 2*MARGIN_W) / IMG_WIDTH - MARGIN_W\n",
    "    return v, u\n",
    "\n",
    "## assertion usage\n",
    "REGR_TARGETS = sorted( [\"yaw\",\"pitch_sin\", \"pitch_cos\", \"roll\", \"udiff\", \"vdiff\", \"z\"] )\n",
    "def _regr_preprocess(regr_dict, vdiff, udiff):\n",
    "    \"\"\" vdiff(h orientation), udiff is regression target \"\"\"\n",
    "    regr_dict[\"vdiff\"] = vdiff\n",
    "    regr_dict[\"udiff\"] = udiff\n",
    "\n",
    "    # Roll\n",
    "    regr_dict['roll'] = rotate(regr_dict['roll'], np.pi)\n",
    "    \n",
    "    # Pitch\n",
    "    regr_dict['pitch_sin'] = sin(regr_dict['pitch'])\n",
    "    regr_dict['pitch_cos'] = cos(regr_dict['pitch'])\n",
    "\n",
    "    # Regress log(Z)\n",
    "    regr_dict[\"z\"] = np.log(regr_dict[\"z\"])\n",
    "    \n",
    "    regr_dict.pop('x')\n",
    "    regr_dict.pop('y')\n",
    "    regr_dict.pop('pitch')\n",
    "    regr_dict.pop('id')\n",
    "    return regr_dict\n",
    "\n",
    "def _regr_back(regr_dict, mask_V_pos, mask_U_pos):\n",
    "    # convert log(z) back to z\n",
    "    regr_dict[\"z\"] = np.exp(regr_dict[\"z\"])\n",
    "\n",
    "    _v, _u = maskVU2VU( mask_V_pos + regr_dict[\"vdiff\"], mask_U_pos + regr_dict[\"udiff\"] )\n",
    "    regr_dict[\"x\"], regr_dict[\"y\"] = UVZ2XY(_u, _v, regr_dict[\"z\"])\n",
    "\n",
    "    regr_dict['roll'] = rotate(regr_dict['roll'], -np.pi)\n",
    "\n",
    "    ## Pitch\n",
    "    pitch_sin = regr_dict['pitch_sin'] / np.sqrt(regr_dict['pitch_sin']**2 + regr_dict['pitch_cos']**2)\n",
    "    pitch_cos = regr_dict['pitch_cos'] / np.sqrt(regr_dict['pitch_sin']**2 + regr_dict['pitch_cos']**2)\n",
    "    regr_dict['pitch'] = np.arccos(pitch_cos) * np.sign(pitch_sin)\n",
    "    \n",
    "    return regr_dict\n",
    "\n",
    "def preprocess_image(img):\n",
    "    img = img[img.shape[0] // 2:]\n",
    "    bg = np.ones_like(img) * img.mean(1, keepdims=True).astype(img.dtype)\n",
    "    bg = bg[:, :MARGIN_W]\n",
    "    img = np.concatenate([bg, img, bg], 1)\n",
    "    img = cv2.resize(img, (IMG_WIDTH, IMG_HEIGHT))\n",
    "    return (img / 255).astype('float32')\n",
    "def preprocess_mask_image(img):  # 上関数とといっしょに編集するように注意\n",
    "    img = img[img.shape[0] // 2:]\n",
    "    bg = np.zeros_like(img).astype(img.dtype)\n",
    "    bg = bg[:, :img.shape[1] // 4]\n",
    "    img = np.concatenate([bg, img, bg], 1)\n",
    "    img = cv2.resize(img, (IMG_WIDTH, IMG_HEIGHT))  # linear interpolate\n",
    "    return (img / 255).astype('float32')\n",
    "\n",
    "# https://github.com/xingyizhou/CenterNet/blob/819e0d0dde02f7b8cb0644987a8d3a370aa8206a/src/lib/utils/image.py\n",
    "# heatmap: H, W\n",
    "# center : X(w direction), Y(H direction)\n",
    "##################### mu_x = int(center[0] + 0.5) CAUSES BUG ##################################\n",
    "\n",
    "def draw_msra_gaussian(heatmap, center, sigma):\n",
    "    # tmp_size = sigma * 3\n",
    "    tmp_size = np.ceil(sigma * 3).astype(int)  # tmp_size should be int for readability ( and to remove bug ? )\n",
    "    mu_x = int(center[0])\n",
    "    mu_y = int(center[1])\n",
    "    w, h = heatmap.shape[0], heatmap.shape[1]\n",
    "    ul = [int(mu_x - tmp_size), int(mu_y - tmp_size)]\n",
    "    br = [int(mu_x + tmp_size + 1), int(mu_y + tmp_size + 1)]\n",
    "    if ul[0] >= h or ul[1] >= w or br[0] < 0 or br[1] < 0:\n",
    "        return heatmap\n",
    "    size = 2 * tmp_size + 1\n",
    "    x = np.arange(0, size, 1, np.float32)\n",
    "    y = x[:, np.newaxis]\n",
    "    x0 = y0 = size // 2\n",
    "    g = np.exp(- ((x - x0) ** 2 + (y - y0) ** 2) / (2 * sigma ** 2))\n",
    "    g_x = max(0, -ul[0]), min(br[0], h) - ul[0]\n",
    "    g_y = max(0, -ul[1]), min(br[1], w) - ul[1]\n",
    "    img_x = max(0, ul[0]), min(br[0], h)\n",
    "    img_y = max(0, ul[1]), min(br[1], w)\n",
    "    heatmap[img_y[0]:img_y[1], img_x[0]:img_x[1]] = np.maximum(\n",
    "      heatmap[img_y[0]:img_y[1], img_x[0]:img_x[1]],\n",
    "      g[g_y[0]:g_y[1], g_x[0]:g_x[1]])\n",
    "    return heatmap\n",
    "\n",
    "def make_heatmap(m, v_arr, u_arr, z_arr):\n",
    "    for v,u,z in zip(v_arr, u_arr, z_arr):\n",
    "        # sigma = 1000 / 3.  / z / MODEL_SCALE\n",
    "        sigma = 800 / 3.  / z / MODEL_SCALE\n",
    "        m = draw_msra_gaussian(m, (u,v), sigma)\n",
    "    return m\n",
    "        \n",
    "\n",
    "def get_mask_and_regr(img, labels):\n",
    "    mask = np.zeros([IMG_HEIGHT // MODEL_SCALE, IMG_WIDTH // MODEL_SCALE], dtype='float32')\n",
    "    regr = np.zeros([IMG_HEIGHT // MODEL_SCALE, IMG_WIDTH // MODEL_SCALE, 7], dtype='float32')\n",
    "    coords = str2coords(labels)\n",
    "    xs, ys = convert_to_img_coords(labels)\n",
    "    z_arr = [e[\"z\"] for e in coords]\n",
    "    \n",
    "    mask_V_arr_float, mask_U_arr_float = VU2maskVU( ys, xs )\n",
    "\n",
    "    # use floor floowing paper\n",
    "    mask_V_arr = np.floor( mask_V_arr_float ).astype('int')\n",
    "    mask_U_arr = np.floor( mask_U_arr_float ).astype('int')\n",
    "    mask_V_diff = mask_V_arr_float - mask_V_arr\n",
    "    mask_U_diff = mask_U_arr_float - mask_U_arr\n",
    "\n",
    "    # make heatmap\n",
    "    mask = make_heatmap(mask, mask_V_arr, mask_U_arr, z_arr)\n",
    "    \n",
    "    for mask_V,mask_U, vdiff,udiff, regr_dict in zip(mask_V_arr,mask_U_arr,mask_V_diff,mask_U_diff, coords):\n",
    "        if mask_V >= 0 and mask_V < IMG_HEIGHT // MODEL_SCALE and mask_U >= 0 and mask_U < IMG_WIDTH // MODEL_SCALE:\n",
    "            regr_dict = _regr_preprocess(regr_dict, vdiff, udiff)\n",
    "            regr[mask_V, mask_U] = [regr_dict[n] for n in sorted(regr_dict)]\n",
    "    return mask, regr"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "GIS6NAm32d8g"
   },
   "outputs": [],
   "source": [
    "def vis_mask(img, mask):\n",
    "    _mm = np.repeat( np.repeat(mask, 8, axis=0), 8, axis=1 )[:,:, None]\n",
    "    _mm = np.repeat(_mm, 3, axis=-1)\n",
    "    _mm [:,:,1] = 0 ; _mm[:,:,2] = 0\n",
    "    \n",
    "    tmp =  np.clip( 0.8 * img + 0.4 * _mm, 0,1)\n",
    "    tmp[ _mm[:,:,0]==1 ] = [0,1,1]\n",
    "    plt.figure(figsize=(16,16))\n",
    "    plt.imshow( tmp , alpha=0.3)\n",
    "    plt.grid()\n",
    "    plt.show()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "K8jRlkOE2d8j",
    "nbpresent": {
     "id": "79072029-2fef-4ae9-a071-ab86ae083573"
    }
   },
   "source": [
    "# PyTorch Dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "_kg_hide-input": true,
    "_kg_hide-output": true,
    "colab": {},
    "colab_type": "code",
    "id": "V1kPnblM2d8k",
    "nbpresent": {
     "id": "89e7fbe4-09dc-46c2-9498-582a0b959727"
    }
   },
   "outputs": [],
   "source": [
    "class CarDataset(Dataset):\n",
    "    \"\"\"Car dataset.\"\"\"\n",
    "\n",
    "    def __init__(self, dataframe, root_dir, mask_root_dir, training=True):\n",
    "        self.df = dataframe\n",
    "        self.root_dir = root_dir\n",
    "        self.mask_root_dir = mask_root_dir  # ignore mask\n",
    "        self.training = training\n",
    "\n",
    "    def __len__(self):\n",
    "        return len(self.df)\n",
    "\n",
    "    def __getitem__(self, idx):\n",
    "        if torch.is_tensor(idx):\n",
    "            idx = idx.tolist()\n",
    "        \n",
    "        # Get image name\n",
    "        idx, labels = self.df.values[idx]\n",
    "        img_name = self.root_dir.format(idx)\n",
    "        \n",
    "        ## Read image\n",
    "        img0 = cv2.imread(img_name)[:,:,::-1]\n",
    "        img = preprocess_image(img0.astype(float))\n",
    "        img = np.rollaxis(img, 2, 0)\n",
    "        \n",
    "        ## Read ignore mask\n",
    "        ign_img0 = cv2.imread(self.mask_root_dir.format(idx), cv2.IMREAD_GRAYSCALE)\n",
    "        if ign_img0 is None:  # where there is no mask image available\n",
    "            ign_img0 = np.zeros((ORIG_H, ORIG_W), dtype='float32')\n",
    "\n",
    "        ign_img = np.array(ign_img0).astype('float32') / 255.\n",
    "        # ign_img = np.rollaxis(ign_img, 2, 0)\n",
    "        ######################################################\n",
    "\n",
    "        # ignore mask for CNN\n",
    "        ign_img_for_feed = preprocess_mask_image(ign_img0)\n",
    "        ign_img_for_feed = np.expand_dims(ign_img_for_feed, 0)  # h,w -> 1,h,w\n",
    "        \n",
    "        \n",
    "        # Get mask and regression maps\n",
    "        if self.training:\n",
    "            mask, regr = get_mask_and_regr(img0, labels)\n",
    "            regr = np.rollaxis(regr, 2, 0)\n",
    "        else:\n",
    "            mask, regr = 0, 0\n",
    "        \n",
    "        return [img, mask, regr, ign_img, ign_img_for_feed]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "_kg_hide-input": true,
    "colab": {},
    "colab_type": "code",
    "id": "JOOIVu902d8n",
    "nbpresent": {
     "id": "10c1f8eb-c9a7-4338-a49a-94820b68449f"
    }
   },
   "outputs": [],
   "source": [
    "train_images_dir = HOMEDIR + 'train_images/{}.jpg'\n",
    "test_images_dir = HOMEDIR + 'test_images/{}.jpg'\n",
    "train_masks_dir = HOMEDIR + 'train_masks/{}.jpg'\n",
    "test_masks_dir = HOMEDIR + 'test_masks/{}.jpg'\n",
    "\n",
    "df_train, df_dev = train_test_split(dfTrain, test_size=0.1, random_state=1042)\n",
    "df_test = dfTest\n",
    "\n",
    "## Create objects of the class type Dataset -  one for each data set\n",
    "train_dataset = CarDataset(df_train, train_images_dir, train_masks_dir)\n",
    "dev_dataset = CarDataset(df_dev, train_images_dir, train_masks_dir)\n",
    "test_dataset = CarDataset(df_test, test_images_dir, test_masks_dir)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "6O4NTmtQ2d8p",
    "nbpresent": {
     "id": "bbcc6c77-29a9-4794-8fcf-c8c58ac8f935"
    }
   },
   "source": [
    "# PyTorch Model - resneXt WITH mask"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "_kg_hide-input": true,
    "colab": {},
    "colab_type": "code",
    "id": "T-clvDS02d8q",
    "nbpresent": {
     "id": "a14d8f04-8d2c-42ba-88f3-969432e9afec"
    }
   },
   "outputs": [],
   "source": [
    "class double_conv(nn.Module):\n",
    "    '''(conv => BN => ReLU) * 2'''\n",
    "    def __init__(self, in_ch, out_ch):\n",
    "        super(double_conv, self).__init__()\n",
    "        self.conv = nn.Sequential(\n",
    "            nn.Conv2d(in_ch, out_ch, 3, padding=1),\n",
    "            nn.BatchNorm2d(out_ch),\n",
    "            nn.ReLU(inplace=True),\n",
    "            nn.Conv2d(out_ch, out_ch, 3, padding=1),\n",
    "            nn.BatchNorm2d(out_ch),\n",
    "            nn.ReLU(inplace=True)\n",
    "        )\n",
    "\n",
    "    def forward(self, x):\n",
    "        x = self.conv(x)\n",
    "        return x\n",
    "\n",
    "class up(nn.Module):\n",
    "    def __init__(self, in_ch, out_ch, bilinear=True):\n",
    "        super(up, self).__init__()\n",
    "\n",
    "        #  would be a nice idea if the upsampling could be learned too,\n",
    "        #  but my machine do not have enough memory to handle all those weights\n",
    "        if bilinear:\n",
    "            self.up = nn.Upsample(scale_factor=2, mode='bilinear', align_corners=True)\n",
    "        else:\n",
    "            self.up = nn.ConvTranspose2d(in_ch//2, in_ch//2, 2, stride=2)\n",
    "\n",
    "        self.conv = double_conv(in_ch, out_ch)\n",
    "\n",
    "    def forward(self, x1, x2=None):\n",
    "        x1 = self.up(x1)\n",
    "        \n",
    "        # input is CHW\n",
    "        diffY = x2.size()[2] - x1.size()[2]\n",
    "        diffX = x2.size()[3] - x1.size()[3]\n",
    "\n",
    "        x1 = F.pad(x1, (diffX // 2, diffX - diffX//2,\n",
    "                        diffY // 2, diffY - diffY//2))\n",
    "        \n",
    "        # for padding issues, see \n",
    "        # https://github.com/HaiyongJiang/U-Net-Pytorch-Unstructured-Buggy/commit/0e854509c2cea854e247a9c615f175f76fbb2e3a\n",
    "        # https://github.com/xiaopeng-liao/Pytorch-UNet/commit/8ebac70e633bac59fc22bb5195e513d5832fb3bd\n",
    "        \n",
    "        if x2 is not None:\n",
    "            x = torch.cat([x2, x1], dim=1)\n",
    "        else:\n",
    "            x = x1\n",
    "        x = self.conv(x)\n",
    "        return x\n",
    "\n",
    "def get_mesh(batch_size, shape_x, shape_y):\n",
    "    mg_x, mg_y = np.meshgrid(np.linspace(0, 1, shape_y), np.linspace(0, 1, shape_x))\n",
    "    mg_x = np.tile(mg_x[None, None, :, :], [batch_size, 1, 1, 1]).astype('float32')\n",
    "    mg_y = np.tile(mg_y[None, None, :, :], [batch_size, 1, 1, 1]).astype('float32')\n",
    "    mesh = torch.cat([torch.tensor(mg_x).to(device), torch.tensor(mg_y).to(device)], 1)\n",
    "    return mesh"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "fCycU9Qt2d8t",
    "nbpresent": {
     "id": "9d436955-7847-438b-97bd-1ad901351ca6"
    }
   },
   "outputs": [],
   "source": [
    "# https://github.com/pytorch/vision/blob/master/torchvision/models/resnet.py\n",
    "#import torch.nn as nn\n",
    "#import torch.nn.functional as F\n",
    "#import torch.utils.model_zoo as model_zoo\n",
    "\n",
    "#__all__ = ['ResNet', 'resnet18', 'resnet34', 'resnet50', 'resnet101',\n",
    "#           'resnet152', 'resnext50_32x4d', 'resnext101_32x8d',\n",
    "#           'wide_resnet50_2', 'wide_resnet101_2']\n",
    "\n",
    "\n",
    "model_urls = {\n",
    "    'resnet18': 'https://download.pytorch.org/models/resnet18-5c106cde.pth',\n",
    "    'resnet34': 'https://download.pytorch.org/models/resnet34-333f7ec4.pth',\n",
    "    'resnet50': 'https://download.pytorch.org/models/resnet50-19c8e357.pth',\n",
    "    'resnet101': 'https://download.pytorch.org/models/resnet101-5d3b4d8f.pth',\n",
    "    'resnet152': 'https://download.pytorch.org/models/resnet152-b121ed2d.pth',\n",
    "    'resnext50_32x4d': 'https://download.pytorch.org/models/resnext50_32x4d-7cdf4587.pth',\n",
    "    'resnext101_32x8d': 'https://download.pytorch.org/models/resnext101_32x8d-8ba56ff5.pth',\n",
    "    'wide_resnet50_2': 'https://download.pytorch.org/models/wide_resnet50_2-95faca4d.pth',\n",
    "    'wide_resnet101_2': 'https://download.pytorch.org/models/wide_resnet101_2-32ee1156.pth',\n",
    "}\n",
    "\n",
    "\n",
    "def conv3x3(in_planes, out_planes, stride=1, groups=1, dilation=1):\n",
    "    \"\"\"3x3 convolution with padding\"\"\"\n",
    "    return nn.Conv2d(in_planes, out_planes, kernel_size=3, stride=stride,\n",
    "                     padding=dilation, groups=groups, bias=False, dilation=dilation)\n",
    "\n",
    "\n",
    "def conv1x1(in_planes, out_planes, stride=1):\n",
    "    \"\"\"1x1 convolution\"\"\"\n",
    "    return nn.Conv2d(in_planes, out_planes, kernel_size=1, stride=stride, bias=False)\n",
    "\n",
    "\n",
    "class BasicBlock(nn.Module):\n",
    "    expansion = 1\n",
    "    __constants__ = ['downsample']\n",
    "\n",
    "    def __init__(self, inplanes, planes, stride=1, downsample=None, groups=1,\n",
    "                 base_width=64, dilation=1, norm_layer=None):\n",
    "        super(BasicBlock, self).__init__()\n",
    "        if norm_layer is None:\n",
    "            norm_layer = nn.BatchNorm2d\n",
    "        if groups != 1 or base_width != 64:\n",
    "            raise ValueError('BasicBlock only supports groups=1 and base_width=64')\n",
    "        if dilation > 1:\n",
    "            raise NotImplementedError(\"Dilation > 1 not supported in BasicBlock\")\n",
    "        # Both self.conv1 and self.downsample layers downsample the input when stride != 1\n",
    "        self.conv1 = conv3x3(inplanes, planes, stride)\n",
    "        self.bn1 = norm_layer(planes)\n",
    "        self.relu = nn.ReLU(inplace=True)\n",
    "        self.conv2 = conv3x3(planes, planes)\n",
    "        self.bn2 = norm_layer(planes)\n",
    "        self.downsample = downsample\n",
    "        self.stride = stride\n",
    "\n",
    "    def forward(self, x):\n",
    "        identity = x\n",
    "\n",
    "        out = self.conv1(x)\n",
    "        out = self.bn1(out)\n",
    "        out = self.relu(out)\n",
    "\n",
    "        out = self.conv2(out)\n",
    "        out = self.bn2(out)\n",
    "\n",
    "        if self.downsample is not None:\n",
    "            identity = self.downsample(x)\n",
    "\n",
    "        out += identity\n",
    "        out = self.relu(out)\n",
    "\n",
    "        return out\n",
    "\n",
    "\n",
    "class Bottleneck(nn.Module):\n",
    "    expansion = 4\n",
    "    __constants__ = ['downsample']\n",
    "\n",
    "    def __init__(self, inplanes, planes, stride=1, downsample=None, groups=1,\n",
    "                 base_width=64, dilation=1, norm_layer=None):\n",
    "        super(Bottleneck, self).__init__()\n",
    "        if norm_layer is None:\n",
    "            norm_layer = nn.BatchNorm2d\n",
    "        width = int(planes * (base_width / 64.)) * groups\n",
    "        # Both self.conv2 and self.downsample layers downsample the input when stride != 1\n",
    "        self.conv1 = conv1x1(inplanes, width)\n",
    "        self.bn1 = norm_layer(width)\n",
    "        self.conv2 = conv3x3(width, width, stride, groups, dilation)\n",
    "        self.bn2 = norm_layer(width)\n",
    "        self.conv3 = conv1x1(width, planes * self.expansion)\n",
    "        self.bn3 = norm_layer(planes * self.expansion)\n",
    "        self.relu = nn.ReLU(inplace=True)\n",
    "        self.downsample = downsample\n",
    "        self.stride = stride\n",
    "\n",
    "    def forward(self, x):\n",
    "        identity = x\n",
    "\n",
    "        out = self.conv1(x)\n",
    "        out = self.bn1(out)\n",
    "        out = self.relu(out)\n",
    "\n",
    "        out = self.conv2(out)\n",
    "        out = self.bn2(out)\n",
    "        out = self.relu(out)\n",
    "\n",
    "        out = self.conv3(out)\n",
    "        out = self.bn3(out)\n",
    "\n",
    "        if self.downsample is not None:\n",
    "            identity = self.downsample(x)\n",
    "\n",
    "        out += identity\n",
    "        out = self.relu(out)\n",
    "\n",
    "        return out\n",
    "\n",
    "\n",
    "class ResNet(nn.Module):\n",
    "\n",
    "    def __init__(self, block, layers, num_classes=1000, zero_init_residual=False,\n",
    "                 groups=1, width_per_group=64, replace_stride_with_dilation=None,\n",
    "                 norm_layer=None, input_channels=3):\n",
    "        super(ResNet, self).__init__()\n",
    "        if norm_layer is None:\n",
    "            norm_layer = nn.BatchNorm2d\n",
    "        self._norm_layer = norm_layer\n",
    "\n",
    "        self.inplanes = 64\n",
    "        self.dilation = 1\n",
    "        if replace_stride_with_dilation is None:\n",
    "            # each element in the tuple indicates if we should replace\n",
    "            # the 2x2 stride with a dilated convolution instead\n",
    "            replace_stride_with_dilation = [False, False, False]\n",
    "        if len(replace_stride_with_dilation) != 3:\n",
    "            raise ValueError(\"replace_stride_with_dilation should be None \"\n",
    "                             \"or a 3-element tuple, got {}\".format(replace_stride_with_dilation))\n",
    "        self.groups = groups\n",
    "        self.base_width = width_per_group\n",
    "        self.conv1 = nn.Conv2d(input_channels, self.inplanes, kernel_size=7, stride=2, padding=3,\n",
    "                               bias=False)\n",
    "        self.bn1 = norm_layer(self.inplanes)\n",
    "        self.relu = nn.ReLU(inplace=True)\n",
    "        self.maxpool = nn.MaxPool2d(kernel_size=3, stride=2, padding=1)\n",
    "        self.layer1 = self._make_layer(block, 64, layers[0])\n",
    "        self.layer2 = self._make_layer(block, 128, layers[1], stride=2,\n",
    "                                       dilate=replace_stride_with_dilation[0])\n",
    "        self.layer3 = self._make_layer(block, 256, layers[2], stride=2,\n",
    "                                       dilate=replace_stride_with_dilation[1])\n",
    "        self.layer4 = self._make_layer(block, 512, layers[3], stride=2,\n",
    "                                       dilate=replace_stride_with_dilation[2])\n",
    "        self.avgpool = nn.AdaptiveAvgPool2d((1, 1))\n",
    "        self.fc = nn.Linear(512 * block.expansion, num_classes)  #herre\n",
    "\n",
    "        for m in self.modules():\n",
    "            if isinstance(m, nn.Conv2d):\n",
    "                nn.init.kaiming_normal_(m.weight, mode='fan_out', nonlinearity='relu')\n",
    "            elif isinstance(m, (nn.BatchNorm2d, nn.GroupNorm)):\n",
    "                nn.init.constant_(m.weight, 1)\n",
    "                nn.init.constant_(m.bias, 0)\n",
    "\n",
    "        # Zero-initialize the last BN in each residual branch,\n",
    "        # so that the residual branch starts with zeros, and each residual block behaves like an identity.\n",
    "        # This improves the model by 0.2~0.3% according to https://arxiv.org/abs/1706.02677\n",
    "        if zero_init_residual:\n",
    "            for m in self.modules():\n",
    "                if isinstance(m, Bottleneck):\n",
    "                    nn.init.constant_(m.bn3.weight, 0)\n",
    "                elif isinstance(m, BasicBlock):\n",
    "                    nn.init.constant_(m.bn2.weight, 0)\n",
    "\n",
    "    def _make_layer(self, block, planes, blocks, stride=1, dilate=False):\n",
    "        norm_layer = self._norm_layer\n",
    "        downsample = None\n",
    "        previous_dilation = self.dilation\n",
    "        if dilate:\n",
    "            self.dilation *= stride\n",
    "            stride = 1\n",
    "        if stride != 1 or self.inplanes != planes * block.expansion:\n",
    "            downsample = nn.Sequential(\n",
    "                conv1x1(self.inplanes, planes * block.expansion, stride),\n",
    "                norm_layer(planes * block.expansion),\n",
    "            )\n",
    "\n",
    "        layers = []\n",
    "        layers.append(block(self.inplanes, planes, stride, downsample, self.groups,\n",
    "                            self.base_width, previous_dilation, norm_layer))\n",
    "        self.inplanes = planes * block.expansion\n",
    "        for _ in range(1, blocks):\n",
    "            layers.append(block(self.inplanes, planes, groups=self.groups,\n",
    "                                base_width=self.base_width, dilation=self.dilation,\n",
    "                                norm_layer=norm_layer))\n",
    "\n",
    "        return nn.Sequential(*layers)\n",
    "\n",
    "    '''def _forward_impl(self, x):\n",
    "        # See note [TorchScript super()]\n",
    "        x = self.conv1(x)\n",
    "        x = self.bn1(x)\n",
    "        x = self.relu(x)\n",
    "        x = self.maxpool(x)\n",
    "\n",
    "        x = self.layer1(x)\n",
    "        x = self.layer2(x)\n",
    "        x = self.layer3(x)\n",
    "        x = self.layer4(x)\n",
    "\n",
    "        x = self.avgpool(x)\n",
    "        x = torch.flatten(x, 1)\n",
    "        x = self.fc(x)\n",
    "\n",
    "        return x\n",
    "\n",
    "    def forward(self, x):\n",
    "        return self._forward_impl(x)'''\n",
    "\n",
    "    def forward(self, x):\n",
    "        conv1 = F.relu(self.bn1(self.conv1(x)), inplace=True)\n",
    "        conv1 = F.max_pool2d(conv1, 3, stride=2, padding=1)\n",
    "\n",
    "        feats4 = self.layer1(conv1)\n",
    "        feats8 = self.layer2(feats4)\n",
    "        feats16 = self.layer3(feats8)\n",
    "        feats32 = self.layer4(feats16)\n",
    "\n",
    "        return feats8, feats16, feats32\n",
    "\n",
    "def _resnet(arch, block, layers, pretrained, progress, **kwargs):\n",
    "    model = ResNet(block, layers, **kwargs)\n",
    "    if pretrained:\n",
    "        state_dict = load_state_dict_from_url(model_urls[arch],\n",
    "                                              progress=progress)\n",
    "        model.load_state_dict(state_dict)\n",
    "    return model\n",
    "\n",
    "\n",
    "def resnet18(pretrained=False, progress=True, **kwargs):\n",
    "    r\"\"\"ResNet-18 model from\n",
    "    `\"Deep Residual Learning for Image Recognition\" <https://arxiv.org/pdf/1512.03385.pdf>`_\n",
    "    Args:\n",
    "        pretrained (bool): If True, returns a model pre-trained on ImageNet\n",
    "        progress (bool): If True, displays a progress bar of the download to stderr\n",
    "    \"\"\"\n",
    "    return _resnet('resnet18', BasicBlock, [2, 2, 2, 2], pretrained, progress,\n",
    "                   **kwargs)\n",
    "\n",
    "\n",
    "def resnet34(pretrained=False, progress=True, **kwargs):\n",
    "    r\"\"\"ResNet-34 model from\n",
    "    `\"Deep Residual Learning for Image Recognition\" <https://arxiv.org/pdf/1512.03385.pdf>`_\n",
    "    Args:\n",
    "        pretrained (bool): If True, returns a model pre-trained on ImageNet\n",
    "        progress (bool): If True, displays a progress bar of the download to stderr\n",
    "    \"\"\"\n",
    "    return _resnet('resnet34', BasicBlock, [3, 4, 6, 3], pretrained, progress,\n",
    "                   **kwargs)\n",
    "\n",
    "\n",
    "def resnet50(pretrained=False, progress=True, **kwargs):\n",
    "    r\"\"\"ResNet-50 model from\n",
    "    `\"Deep Residual Learning for Image Recognition\" <https://arxiv.org/pdf/1512.03385.pdf>`_\n",
    "    Args:\n",
    "        pretrained (bool): If True, returns a model pre-trained on ImageNet\n",
    "        progress (bool): If True, displays a progress bar of the download to stderr\n",
    "    \"\"\"\n",
    "    return _resnet('resnet50', Bottleneck, [3, 4, 6, 3], pretrained, progress,\n",
    "                   **kwargs)\n",
    "\n",
    "\n",
    "def resnet101(pretrained=False, progress=True, **kwargs):\n",
    "    r\"\"\"ResNet-101 model from\n",
    "    `\"Deep Residual Learning for Image Recognition\" <https://arxiv.org/pdf/1512.03385.pdf>`_\n",
    "    Args:\n",
    "        pretrained (bool): If True, returns a model pre-trained on ImageNet\n",
    "        progress (bool): If True, displays a progress bar of the download to stderr\n",
    "    \"\"\"\n",
    "    return _resnet('resnet101', Bottleneck, [3, 4, 23, 3], pretrained, progress,\n",
    "                   **kwargs)\n",
    "\n",
    "\n",
    "def resnet152(pretrained=False, progress=True, **kwargs):\n",
    "    r\"\"\"ResNet-152 model from\n",
    "    `\"Deep Residual Learning for Image Recognition\" <https://arxiv.org/pdf/1512.03385.pdf>`_\n",
    "    Args:\n",
    "        pretrained (bool): If True, returns a model pre-trained on ImageNet\n",
    "        progress (bool): If True, displays a progress bar of the download to stderr\n",
    "    \"\"\"\n",
    "    return _resnet('resnet152', Bottleneck, [3, 8, 36, 3], pretrained, progress,\n",
    "                   **kwargs)\n",
    "\n",
    "\n",
    "def resnext50_32x4d(pretrained=False, progress=True, **kwargs):\n",
    "    r\"\"\"ResNeXt-50 32x4d model from\n",
    "    `\"Aggregated Residual Transformation for Deep Neural Networks\" <https://arxiv.org/pdf/1611.05431.pdf>`_\n",
    "    Args:\n",
    "        pretrained (bool): If True, returns a model pre-trained on ImageNet\n",
    "        progress (bool): If True, displays a progress bar of the download to stderr\n",
    "    \"\"\"\n",
    "    kwargs['groups'] = 32\n",
    "    kwargs['width_per_group'] = 4\n",
    "    return _resnet('resnext50_32x4d', Bottleneck, [3, 4, 6, 3],\n",
    "                   pretrained, progress, **kwargs)\n",
    "\n",
    "\n",
    "def resnext101_32x8d(pretrained=False, progress=True, **kwargs):\n",
    "    r\"\"\"ResNeXt-101 32x8d model from\n",
    "    `\"Aggregated Residual Transformation for Deep Neural Networks\" <https://arxiv.org/pdf/1611.05431.pdf>`_\n",
    "    Args:\n",
    "        pretrained (bool): If True, returns a model pre-trained on ImageNet\n",
    "        progress (bool): If True, displays a progress bar of the download to stderr\n",
    "    \"\"\"\n",
    "    kwargs['groups'] = 32\n",
    "    kwargs['width_per_group'] = 8\n",
    "    return _resnet('resnext101_32x8d', Bottleneck, [3, 4, 23, 3],\n",
    "                   pretrained, progress, **kwargs)\n",
    "\n",
    "\n",
    "def wide_resnet50_2(pretrained=False, progress=True, **kwargs):\n",
    "    r\"\"\"Wide ResNet-50-2 model from\n",
    "    `\"Wide Residual Networks\" <https://arxiv.org/pdf/1605.07146.pdf>`_\n",
    "    The model is the same as ResNet except for the bottleneck number of channels\n",
    "    which is twice larger in every block. The number of channels in outer 1x1\n",
    "    convolutions is the same, e.g. last block in ResNet-50 has 2048-512-2048\n",
    "    channels, and in Wide ResNet-50-2 has 2048-1024-2048.\n",
    "    Args:\n",
    "        pretrained (bool): If True, returns a model pre-trained on ImageNet\n",
    "        progress (bool): If True, displays a progress bar of the download to stderr\n",
    "    \"\"\"\n",
    "    kwargs['width_per_group'] = 64 * 2\n",
    "    return _resnet('wide_resnet50_2', Bottleneck, [3, 4, 6, 3],\n",
    "                   pretrained, progress, **kwargs)\n",
    "\n",
    "\n",
    "def wide_resnet101_2(pretrained=False, progress=True, **kwargs):\n",
    "    r\"\"\"Wide ResNet-101-2 model from\n",
    "    `\"Wide Residual Networks\" <https://arxiv.org/pdf/1605.07146.pdf>`_\n",
    "    The model is the same as ResNet except for the bottleneck number of channels\n",
    "    which is twice larger in every block. The number of channels in outer 1x1\n",
    "    convolutions is the same, e.g. last block in ResNet-50 has 2048-512-2048\n",
    "    channels, and in Wide ResNet-50-2 has 2048-1024-2048.\n",
    "    Args:\n",
    "        pretrained (bool): If True, returns a model pre-trained on ImageNet\n",
    "        progress (bool): If True, displays a progress bar of the download to stderr\n",
    "    \"\"\"\n",
    "    kwargs['width_per_group'] = 64 * 2\n",
    "    return _resnet('wide_resnet101_2', Bottleneck, [3, 4, 23, 3],\n",
    "                   pretrained, progress, **kwargs)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "vZmyQW7X2d8w"
   },
   "outputs": [],
   "source": [
    "#base_model = resnext50_32x4d(pretrained=True)\n",
    "#base_model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "6NMNMiit2d8z",
    "nbpresent": {
     "id": "785e5bb6-8333-42bc-9275-e2bd63dff890"
    }
   },
   "outputs": [],
   "source": [
    "class CentResnet(nn.Module):\n",
    "    '''Mixture of previous classes'''\n",
    "    def __init__(self, n_classes):\n",
    "        super(CentResnet, self).__init__()\n",
    "        self.base_model = resnext50_32x4d(pretrained=False, input_channels=6)\n",
    "        \n",
    "        # Lateral layers convert resnet outputs to a common feature size\n",
    "        self.lat8 = nn.Conv2d(512, 256, 1)\n",
    "        self.lat16 = nn.Conv2d(1024, 256, 1)\n",
    "        self.lat32 = nn.Conv2d(2048, 256, 1)\n",
    "        self.bn8 = nn.GroupNorm(16, 256)\n",
    "        self.bn16 = nn.GroupNorm(16, 256)\n",
    "        self.bn32 = nn.GroupNorm(16, 256)\n",
    "\n",
    "        if USEMASK:\n",
    "            self.conv0 = double_conv(5 + 1, 64)\n",
    "        else:\n",
    "            self.conv0 = double_conv(5, 64)\n",
    "            \n",
    "        self.conv1 = double_conv(64, 128)\n",
    "        self.conv2 = double_conv(128, 512)\n",
    "        self.conv3 = double_conv(512, 1024)\n",
    "        \n",
    "        self.mp = nn.MaxPool2d(2)\n",
    "        \n",
    "        self.up1 = up(1282 , 512) #+ 1024\n",
    "        self.up2 = up(512 + 512, 256)\n",
    "        self.outc = nn.Conv2d(256, n_classes, 1)\n",
    "        \n",
    "    \n",
    "    def forward(self, x):\n",
    "        batch_size = x.shape[0]\n",
    "        mesh1 = get_mesh(batch_size, x.shape[2], x.shape[3])\n",
    "        x0 = torch.cat([x, mesh1], 1)\n",
    "        x1 = self.mp(self.conv0(x0))\n",
    "        x2 = self.mp(self.conv1(x1))\n",
    "        x3 = self.mp(self.conv2(x2))\n",
    "        x4 = self.mp(self.conv3(x3))\n",
    "        \n",
    "        #feats = self.base_model.extract_features(x)\n",
    "                # Run frontend network\n",
    "        if USEMASK:\n",
    "            ## feats8, feats16, feats32 = self.base_model(x[:,0:3])  ## use first 3 channel. this may not be proper way\n",
    "            feats8, feats16, feats32 = self.base_model(x0)  ## C=6 : rgb(3)+mask(1)+mesh(2)\n",
    "        else:\n",
    "            feats8, feats16, feats32 = self.base_model(x)\n",
    "\n",
    "        lat8 = F.relu(self.bn8(self.lat8(feats8)))\n",
    "        lat16 = F.relu(self.bn16(self.lat16(feats16)))\n",
    "        lat32 = F.relu(self.bn32(self.lat32(feats32)))\n",
    "        \n",
    "        # Add positional info\n",
    "        mesh2 = get_mesh(batch_size, lat32.shape[2], lat32.shape[3])\n",
    "        feats = torch.cat([lat32, mesh2], 1)\n",
    "        #print(feats.shape)\n",
    "        #print (x4.shape)\n",
    "        x = self.up1(feats, x4)\n",
    "        x = self.up2(x, x3)\n",
    "        x = self.outc(x)\n",
    "        return x"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "1oPIuDPP2d82",
    "nbpresent": {
     "id": "4a74a8a2-7c49-455a-a66e-9951011f89f3"
    }
   },
   "outputs": [],
   "source": [
    "def infer_image(img, ign_mask_for_feed):  # shape:[B,C,H,W]\n",
    "    return model( torch.from_numpy( np.concatenate((img, ign_mask_for_feed),axis=1) ).to(device) )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "zxirCiRr2d84"
   },
   "outputs": [],
   "source": [
    "def sigmoid(x):\n",
    "    return 1 / (1 + np.exp(-x))\n",
    "\n",
    "def postprocess_heatmap(logits, thresh=0.45):\n",
    "    prob = sigmoid(logits)\n",
    "    mp2d = torch.nn.MaxPool2d(3, stride=1, padding=1, dilation=1, return_indices=False, ceil_mode=False)\n",
    "    out = mp2d( torch.Tensor([[prob]]) ).numpy()[0][0]\n",
    "    return (prob == out) & (prob > thresh)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "tiNGzFtO2d87",
    "nbpresent": {
     "id": "b6792e19-dd80-4de3-b4b4-10d135281a28"
    }
   },
   "outputs": [],
   "source": [
    "DISTANCE_THRESH_CLEAR = 2\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "9pVmmuT02d8-"
   },
   "outputs": [],
   "source": [
    "def clear_duplicates(coords):\n",
    "    for c1 in coords:\n",
    "        xyz1 = np.array([c1['x'], c1['y'], c1['z']])\n",
    "        for c2 in coords:\n",
    "            xyz2 = np.array([c2['x'], c2['y'], c2['z']])\n",
    "            distance = np.sqrt(((xyz1 - xyz2)**2).sum())\n",
    "            if distance < DISTANCE_THRESH_CLEAR:\n",
    "                if c1['confidence'] < c2['confidence']:\n",
    "                    c1['confidence'] = -1\n",
    "    return [c for c in coords if c['confidence'] > 0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "KKUbOOXv2d9C"
   },
   "outputs": [],
   "source": [
    "def extract_coords(prediction, ign_mask):\n",
    "    assert ign_mask.shape[0] == ORIG_H   #\n",
    "    logits = prediction[0]\n",
    "    regr_output = prediction[1:]\n",
    "    points_mat = postprocess_heatmap(logits) \n",
    "    points = np.argwhere( points_mat > 0 )\n",
    "    \n",
    "    col_names = sorted(REGR_TARGETS)  # vdiff,udiff,z,yaw,pitch_sin,pitch_cos,roll\n",
    "    coords = []\n",
    "    for r, c in points:           \n",
    "        regr_dict = dict(zip(col_names, regr_output[:, r, c]))\n",
    "        \n",
    "        # use heatmap-peak (V,U) position\n",
    "        regr_backed = _regr_back(regr_dict, r, c)\n",
    "        \n",
    "        _U, _V = XYZ2UV(regr_backed[\"x\"], regr_backed[\"y\"], regr_backed[\"z\"])\n",
    "        _U, _V = int(_U), int(_V)\n",
    "        if _V>=0 and _V<ORIG_H and _U>=0 and _U<ORIG_W and ign_mask[_V,_U] > 0.5:  # floor(u), floor(v)\n",
    "            # print(\"point is in ignore_mask\")\n",
    "            continue\n",
    "\n",
    "        coords.append(regr_backed)\n",
    "        coords[-1]['confidence'] = 1 / (1 + np.exp(-logits[r, c]))\n",
    "\n",
    "        coords = clear_duplicates(coords)\n",
    "    return coords"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "HARev-oB2d9E"
   },
   "source": [
    "# Load each model and find prediction time"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 105
    },
    "colab_type": "code",
    "id": "Hr755YrK2d9F",
    "outputId": "8fd93239-a571-4ee0-d1fe-f359551a742b"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2020-03-10 14:31:42.982269\n",
      "2020-03-10 14:31:42.983377\n",
      "0:00:00.001108\n",
      "<class 'datetime.timedelta'>\n",
      "0:00:00.000554\n"
     ]
    }
   ],
   "source": [
    "import datetime\n",
    "time_start = datetime.datetime.now()\n",
    "print(time_start)\n",
    "time_end = datetime.datetime.now()\n",
    "print(time_end)\n",
    "\n",
    "time_delta = time_end - time_start\n",
    "\n",
    "print(time_delta)\n",
    "print(type(time_delta))\n",
    "\n",
    "time_avg = time_delta / 2\n",
    "print(time_avg)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 34
    },
    "colab_type": "code",
    "id": "WcrRusDY2d9I",
    "outputId": "f3691e1a-3460-4503-e39d-c10796c6a291"
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0"
      ]
     },
     "execution_count": 33,
     "metadata": {
      "tags": []
     },
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import datetime\n",
    "\n",
    "import gc\n",
    "torch.cuda.empty_cache()\n",
    "gc.collect()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 107
    },
    "colab_type": "code",
    "id": "OYoEcOXu2d9L",
    "outputId": "fa5c0150-c9bc-43de-b98a-2bb4e26396c3"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['model_rohit_resneXt_ep4_bct_Mask_20200308_0526.pth', 'model_manmeet_resneXt_ep4_focal_Mask_20200310_0600.pth']\n",
      "\n",
      "\n",
      "{'model_rohit_resneXt_ep4_bct_Mask_20200308_0526.pth': '/content/drive/My Drive/baidu/Models/Cent-ResneXt_WITHmask/model_rohit_resneXt_ep4_bct_Mask_20200308_0526.pth', 'model_manmeet_resneXt_ep4_focal_Mask_20200310_0600.pth': '/content/drive/My Drive/baidu/Models/Cent-ResneXt_WITHmask/model_manmeet_resneXt_ep4_focal_Mask_20200310_0600.pth'}\n"
     ]
    }
   ],
   "source": [
    "print(f\"{os.listdir(model_path_dir)}\\n\\n\")\n",
    "dict_models = {}\n",
    "for eachmodel in os.listdir(model_path_dir):\n",
    "    dict_models[eachmodel] = model_path_dir + eachmodel\n",
    "\n",
    "print(f\"{dict_models}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 34
    },
    "colab_type": "code",
    "id": "4AUDHei-2d9N",
    "outputId": "52c7c483-9644-4c2b-dccc-5164a39be795",
    "scrolled": true
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'/content/drive/My Drive/baidu/Models/Cent-ResneXt_WITHmask/model_rohit_resneXt_ep4_bct_Mask_20200308_0526.pth'"
      ]
     },
     "execution_count": 35,
     "metadata": {
      "tags": []
     },
     "output_type": "execute_result"
    }
   ],
   "source": [
    "dict_models.pop('model_rohit_resneXt_ep4_bct_Mask_20200308_0526.pth')\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "X7WjONOK2d9Q"
   },
   "outputs": [],
   "source": [
    "## running only 1st model = 'model_manmeet_resneXt_ep4_focal_Mask_20200310_0600.pth'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 34
    },
    "colab_type": "code",
    "id": "SIcA_vRa2d9T",
    "outputId": "82f9c102-d221-474f-a18e-81432c8f8d75"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'model_manmeet_resneXt_ep4_focal_Mask_20200310_0600.pth': '/content/drive/My Drive/baidu/Models/Cent-ResneXt_WITHmask/model_manmeet_resneXt_ep4_focal_Mask_20200310_0600.pth'}\n"
     ]
    }
   ],
   "source": [
    "print(f\"{dict_models}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 34
    },
    "colab_type": "code",
    "id": "aktrpRyF2d9V",
    "outputId": "151ac628-ba28-468d-fc93-9356810aeea4"
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "426"
      ]
     },
     "execution_count": 38,
     "metadata": {
      "tags": []
     },
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(df_dev)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 336
    },
    "colab_type": "code",
    "id": "CGPLCYhL2d9Y",
    "outputId": "6faaa104-e187-475a-aa17-e0041b108c82",
    "scrolled": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "USEMASK set as = True\n",
      "\n",
      "\n",
      "NUMBER_IMAGES_TO_PREDICT = 426\n",
      "\n",
      "\n",
      "--------- For model number #1 ---------\n",
      "\n",
      "model = model_manmeet_resneXt_ep4_focal_Mask_20200310_0600.pth\n",
      "Total time_delta = 0:15:41.856196\n",
      "average time per image = 0:00:02.210930\n",
      "\n",
      "picked from location = /content/drive/My Drive/baidu/Models/Cent-ResneXt_WITHmask/model_manmeet_resneXt_ep4_focal_Mask_20200310_0600.pth\n",
      "\n",
      "\n",
      "DONE\n",
      "\n",
      "\n"
     ]
    }
   ],
   "source": [
    "print(f\"USEMASK set as = {USEMASK}\\n\\n\")\n",
    "\n",
    "#NUMBER_IMAGES_TO_PREDICT = 2  # for testing\n",
    "NUMBER_IMAGES_TO_PREDICT = len(df_dev)  # uncomment to use full 426 values of dev\n",
    "\n",
    "print(f\"NUMBER_IMAGES_TO_PREDICT = {NUMBER_IMAGES_TO_PREDICT}\\n\")\n",
    "\n",
    "time_start = None\n",
    "time_end = None\n",
    "time_delta = None\n",
    "time_avg = None\n",
    "\n",
    "model_idx=1\n",
    "\n",
    "for each_model_name, each_model_path in dict_models.items():\n",
    "    #\n",
    "    print(f\"\\n--------- For model number #{model_idx} ---------\\n\")\n",
    "    #\n",
    "    torch.cuda.empty_cache()\n",
    "    gc.collect()\n",
    "    model = None\n",
    "    saved_model = None\n",
    "    time_start = None\n",
    "    time_end = None\n",
    "    time_avg = None\n",
    "    #\n",
    "    ## Load the saved model\n",
    "    saved_model = CentResnet(8).to(device)\n",
    "    saved_model.load_state_dict(torch.load(each_model_path))\n",
    "    model = saved_model\n",
    "    #saved_model.eval()\n",
    "    #type(model)\n",
    "    #\n",
    "    time_start = datetime.datetime.now()\n",
    "    for idx in range(NUMBER_IMAGES_TO_PREDICT):\n",
    "        img, heatmap, regr, ign_mask, ign_mask_for_feed = dev_dataset[idx]\n",
    "        mask = (heatmap >= 1).astype(float)\n",
    "    \n",
    "        if USEMASK:\n",
    "            output = infer_image(img[None], ign_mask_for_feed[None])\n",
    "            output = output.data.cpu().numpy()\n",
    "        else:\n",
    "            output = model(torch.tensor(img[None]).to(device)).data.cpu().numpy()\n",
    "    #\n",
    "    time_end = datetime.datetime.now()\n",
    "    time_delta = time_end - time_start\n",
    "    time_avg = time_delta / NUMBER_IMAGES_TO_PREDICT\n",
    "    \n",
    "    print(f\"model = {each_model_name}\\nTotal time_delta = {time_delta}\\naverage time per image = {time_avg}\\n\\npicked from location = {each_model_path}\")\n",
    "    \n",
    "    model_idx += 1\n",
    "\n",
    "print(f\"\\n\\nDONE\\n\\n\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "Ok03v-002d9c"
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "accelerator": "GPU",
  "colab": {
   "collapsed_sections": [
    "hHY2Kdin2d8C",
    "EjxezRmK2d8O",
    "YV3jBhGE2d8X",
    "DqeJqXNo2d8b",
    "K8jRlkOE2d8j",
    "6O4NTmtQ2d8p"
   ],
   "name": "PREDICTION_resneXt_WITHmask_time_error_comparison-1_m1.ipynb",
   "provenance": []
  },
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}
